Run training...
Run training...
Run training...
2023-08-29 12:55:11.477704: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2023-08-29 12:55:11.487738: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2023-08-29 12:55:11.495021: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
[1,7]<stdout>:Experiment dir : LM-TFM-wt103/20230830-005516
[1,7]<stdout>:Loading cached dataset...
[1,4]<stdout>:Experiment dir : LM-TFM-wt103/20230830-005516
[1,4]<stdout>:Loading cached dataset...
[1,2]<stdout>:Experiment dir : LM-TFM-wt103/20230829-125516
[1,2]<stdout>:Loading cached dataset...
[1,3]<stdout>:Experiment dir : LM-TFM-wt103/20230830-005516
[1,3]<stdout>:Loading cached dataset...
[1,6]<stdout>:Experiment dir : LM-TFM-wt103/20230829-125516
[1,6]<stdout>:Loading cached dataset...
[1,1]<stdout>:Experiment dir : LM-TFM-wt103/20230830-005516
[1,1]<stdout>:Loading cached dataset...
[1,0]<stdout>:Experiment dir : LM-TFM-wt103/20230829-125516
[1,0]<stdout>:Loading cached dataset...
[1,5]<stdout>:Experiment dir : LM-TFM-wt103/20230829-125516
[1,5]<stdout>:Loading cached dataset...
[1,5]<stdout>:Experiment dir : LM-TFM-wt103/20230829-125516
[1,5]<stdout>:Loading cached dataset...
[1,1]<stdout>:Experiment dir : LM-TFM-wt103/20230830-005516
[1,1]<stdout>:Loading cached dataset...
[1,3]<stdout>:Experiment dir : LM-TFM-wt103/20230830-005516
[1,3]<stdout>:Loading cached dataset...
[1,0]<stdout>:Experiment dir : LM-TFM-wt103/20230829-125516
[1,0]<stdout>:Loading cached dataset...
[1,4]<stdout>:Experiment dir : LM-TFM-wt103/20230830-005516
[1,7]<stdout>:Experiment dir : LM-TFM-wt103/20230830-005516
[1,6]<stdout>:Experiment dir : LM-TFM-wt103/20230829-125516
[1,4]<stdout>:Loading cached dataset...
[1,7]<stdout>:Loading cached dataset...
[1,6]<stdout>:Loading cached dataset...
[1,2]<stdout>:Experiment dir : LM-TFM-wt103/20230829-125516
[1,2]<stdout>:Loading cached dataset...
[1,5]<stdout>:Experiment dir : LM-TFM-wt103/20230829-125516
[1,5]<stdout>:Loading cached dataset...
[1,0]<stdout>:Experiment dir : LM-TFM-wt103/20230829-125516
[1,0]<stdout>:Loading cached dataset...
[1,6]<stdout>:Experiment dir : LM-TFM-wt103/20230829-125516
[1,6]<stdout>:Loading cached dataset...
[1,4]<stdout>:Experiment dir : LM-TFM-wt103/20230830-005516
[1,4]<stdout>:Loading cached dataset...
[1,2]<stdout>:Experiment dir : LM-TFM-wt103/20230829-125516
[1,7]<stdout>:Experiment dir : LM-TFM-wt103/20230830-005516
[1,2]<stdout>:Loading cached dataset...
[1,7]<stdout>:Loading cached dataset...
[1,3]<stdout>:Experiment dir : LM-TFM-wt103/20230830-005516
[1,1]<stdout>:Experiment dir : LM-TFM-wt103/20230830-005516
[1,3]<stdout>:Loading cached dataset...
[1,1]<stdout>:Loading cached dataset...
[1,0]<stdout>:33.13504147529602
[1,0]<stdout>:| epoch   1 step      100 |    100 batches | lr 0.00025 | ms/batch 1835.60 | loss  6.68 | ppl   793.474
[1,0]<stdout>:----------------------------------------------------------------------------------------------------
[1,0]<stdout>:| Eval   4 at step      100 | time: 39.50s | valid loss  6.47 | valid ppl   646.151
[1,0]<stdout>:----------------------------------------------------------------------------------------------------
[1,5]<stdout>:====================================================================================================
[1,5]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,5]<stdout>:    - dataset : wt103
[1,5]<stdout>:    - n_layer : 16
[1,5]<stdout>:    - n_head : 10
[1,5]<stdout>:    - d_head : 41
[1,5]<stdout>:    - d_embed : 410
[1,5]<stdout>:    - d_model : 410
[1,5]<stdout>:    - d_inner : 2100
[1,5]<stdout>:    - dropout : 0.1
[1,5]<stdout>:    - dropatt : 0.0
[1,5]<stdout>:    - init : normal
[1,5]<stdout>:    - emb_init : normal
[1,5]<stdout>:    - init_range : 0.1
[1,5]<stdout>:    - emb_init_range : 0.01
[1,5]<stdout>:    - init_std : 0.02
[1,5]<stdout>:    - proj_init_std : 0.01
[1,5]<stdout>:    - optim : adam
[1,5]<stdout>:    - lr : 0.00025
[1,5]<stdout>:    - mom : 0.0
[1,5]<stdout>:    - scheduler : cosine
[1,5]<stdout>:    - warmup_step : 0
[1,5]<stdout>:    - decay_rate : 0.5
[1,5]<stdout>:    - lr_min : 0.0
[1,5]<stdout>:    - clip : 0.25
[1,5]<stdout>:    - clip_nonemb : False
[1,5]<stdout>:    - max_step : 5000
[1,5]<stdout>:    - batch_size : 60
[1,5]<stdout>:    - batch_chunk : 1
[1,5]<stdout>:    - tgt_len : 150
[1,5]<stdout>:    - eval_tgt_len : 150
[1,5]<stdout>:    - ext_len : 0
[1,5]<stdout>:    - mem_len : 150
[1,5]<stdout>:    - not_tied : False
[1,5]<stdout>:    - seed : 1111
[1,5]<stdout>:    - cuda : True
[1,5]<stdout>:    - adaptive : True
[1,5]<stdout>:    - div_val : 1
[1,5]<stdout>:    - pre_lnorm : False
[1,5]<stdout>:    - varlen : False
[1,5]<stdout>:    - multi_gpu : False
[1,5]<stdout>:    - log_interval : 25
[1,5]<stdout>:    - eval_interval : 25
[1,5]<stdout>:    - work_dir : LM-TFM-wt103/20230829-125516
[1,5]<stdout>:    - restart : False
[1,5]<stdout>:    - restart_dir : 
[1,5]<stdout>:    - debug : False
[1,5]<stdout>:    - same_length : False
[1,5]<stdout>:    - attn_type : 0
[1,5]<stdout>:    - clamp_len : -1
[1,5]<stdout>:    - eta_min : 0.0
[1,5]<stdout>:    - gpu0_bsz : 4
[1,5]<stdout>:    - max_eval_steps : -1
[1,5]<stdout>:    - sample_softmax : -1
[1,5]<stdout>:    - patience : 0
[1,5]<stdout>:    - finetune_v2 : False
[1,5]<stdout>:    - finetune_v3 : False
[1,5]<stdout>:    - fp16 : False
[1,5]<stdout>:    - static_loss_scale : 1
[1,5]<stdout>:    - dynamic_loss_scale : False
[1,5]<stdout>:    - tied : True
[1,5]<stdout>:    - n_token : 267735
[1,5]<stdout>:    - n_all_param : 151107538
[1,5]<stdout>:    - n_nonemb_param : 41066400
[1,5]<stdout>:====================================================================================================
[1,5]<stdout>:#params = 151107538
[1,5]<stdout>:#non emb params = 41066400
[1,4]<stdout>:====================================================================================================
[1,4]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,4]<stdout>:    - dataset : wt103
[1,4]<stdout>:    - n_layer : 16
[1,4]<stdout>:    - n_head : 10
[1,4]<stdout>:    - d_head : 41
[1,4]<stdout>:    - d_embed : 410
[1,4]<stdout>:    - d_model : 410
[1,4]<stdout>:    - d_inner : 2100
[1,4]<stdout>:    - dropout : 0.1
[1,4]<stdout>:    - dropatt : 0.0
[1,4]<stdout>:    - init : normal
[1,4]<stdout>:    - emb_init : normal
[1,4]<stdout>:    - init_range : 0.1
[1,4]<stdout>:    - emb_init_range : 0.01
[1,4]<stdout>:    - init_std : 0.02
[1,4]<stdout>:    - proj_init_std : 0.01
[1,4]<stdout>:    - optim : adam
[1,4]<stdout>:    - lr : 0.00025
[1,4]<stdout>:    - mom : 0.0
[1,4]<stdout>:    - scheduler : cosine
[1,4]<stdout>:    - warmup_step : 0
[1,4]<stdout>:    - decay_rate : 0.5
[1,4]<stdout>:    - lr_min : 0.0
[1,4]<stdout>:    - clip : 0.25
[1,4]<stdout>:    - clip_nonemb : False
[1,4]<stdout>:    - max_step : 5000
[1,4]<stdout>:    - batch_size : 60
[1,4]<stdout>:    - batch_chunk : 1
[1,4]<stdout>:    - tgt_len : 150
[1,2]<stdout>:====================================================================================================
[1,4]<stdout>:    - eval_tgt_len : 150
[1,4]<stdout>:    - ext_len : 0
[1,4]<stdout>:    - mem_len : 150
[1,4]<stdout>:    - not_tied : False
[1,4]<stdout>:    - seed : 1111
[1,4]<stdout>:    - cuda : True
[1,4]<stdout>:    - adaptive : True
[1,4]<stdout>:    - div_val : 1
[1,4]<stdout>:    - pre_lnorm : False
[1,4]<stdout>:    - varlen : False
[1,4]<stdout>:    - multi_gpu : False
[1,4]<stdout>:    - log_interval : 25
[1,2]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,4]<stdout>:    - eval_interval : 25
[1,4]<stdout>:    - work_dir : LM-TFM-wt103/20230830-005516
[1,4]<stdout>:    - restart : False
[1,2]<stdout>:    - dataset : wt103
[1,2]<stdout>:    - n_layer : 16
[1,2]<stdout>:    - n_head : 10
[1,2]<stdout>:    - d_head : 41
[1,2]<stdout>:    - d_embed : 410
[1,4]<stdout>:    - restart_dir : 
[1,2]<stdout>:    - d_model : 410
[1,4]<stdout>:    - debug : False
[1,4]<stdout>:    - same_length : False
[1,4]<stdout>:    - attn_type : 0
[1,2]<stdout>:    - d_inner : 2100
[1,2]<stdout>:    - dropout : 0.1
[1,2]<stdout>:    - dropatt : 0.0
[1,2]<stdout>:    - init : normal
[1,4]<stdout>:    - clamp_len : -1
[1,4]<stdout>:    - eta_min : 0.0
[1,2]<stdout>:    - emb_init : normal
[1,2]<stdout>:    - init_range : 0.1
[1,2]<stdout>:    - emb_init_range : 0.01
[1,2]<stdout>:    - init_std : 0.02
[1,4]<stdout>:    - gpu0_bsz : 4
[1,4]<stdout>:    - max_eval_steps : -1
[1,4]<stdout>:    - sample_softmax : -1
[1,2]<stdout>:    - proj_init_std : 0.01
[1,2]<stdout>:    - optim : adam
[1,2]<stdout>:    - lr : 0.00025
[1,2]<stdout>:    - mom : 0.0
[1,4]<stdout>:    - patience : 0
[1,2]<stdout>:    - scheduler : cosine
[1,2]<stdout>:    - warmup_step : 0
[1,2]<stdout>:    - decay_rate : 0.5
[1,4]<stdout>:    - finetune_v2 : False
[1,4]<stdout>:    - finetune_v3 : False
[1,4]<stdout>:    - fp16 : False
[1,4]<stdout>:    - static_loss_scale : 1
[1,4]<stdout>:    - dynamic_loss_scale : False
[1,2]<stdout>:    - lr_min : 0.0
[1,2]<stdout>:    - clip : 0.25
[1,4]<stdout>:    - tied : True
[1,4]<stdout>:    - n_token : 267735
[1,2]<stdout>:    - clip_nonemb : False
[1,2]<stdout>:    - max_step : 5000
[1,2]<stdout>:    - batch_size : 60
[1,4]<stdout>:    - n_all_param : 151107538
[1,4]<stdout>:    - n_nonemb_param : 41066400
[1,4]<stdout>:====================================================================================================
[1,4]<stdout>:#params = 151107538
[1,4]<stdout>:#non emb params = 41066400
[1,2]<stdout>:    - batch_chunk : 1
[1,2]<stdout>:    - tgt_len : 150
[1,2]<stdout>:    - eval_tgt_len : 150
[1,2]<stdout>:    - ext_len : 0
[1,2]<stdout>:    - mem_len : 150
[1,2]<stdout>:    - not_tied : False
[1,2]<stdout>:    - seed : 1111
[1,2]<stdout>:    - cuda : True
[1,2]<stdout>:    - adaptive : True
[1,2]<stdout>:    - div_val : 1
[1,2]<stdout>:    - pre_lnorm : False
[1,2]<stdout>:    - varlen : False
[1,2]<stdout>:    - multi_gpu : False
[1,2]<stdout>:    - log_interval : 25
[1,2]<stdout>:    - eval_interval : 25
[1,2]<stdout>:    - work_dir : LM-TFM-wt103/20230829-125516
[1,2]<stdout>:    - restart : False
[1,2]<stdout>:    - restart_dir : 
[1,2]<stdout>:    - debug : False
[1,2]<stdout>:    - same_length : False
[1,2]<stdout>:    - attn_type : 0
[1,2]<stdout>:    - clamp_len : -1
[1,2]<stdout>:    - eta_min : 0.0
[1,2]<stdout>:    - gpu0_bsz : 4
[1,2]<stdout>:    - max_eval_steps : -1
[1,2]<stdout>:    - sample_softmax : -1
[1,2]<stdout>:    - patience : 0
[1,2]<stdout>:    - finetune_v2 : False
[1,2]<stdout>:    - finetune_v3 : False
[1,2]<stdout>:    - fp16 : False
[1,2]<stdout>:    - static_loss_scale : 1
[1,2]<stdout>:    - dynamic_loss_scale : False
[1,2]<stdout>:    - tied : True
[1,2]<stdout>:    - n_token : 267735
[1,2]<stdout>:    - n_all_param : 151107538
[1,2]<stdout>:    - n_nonemb_param : 41066400
[1,2]<stdout>:====================================================================================================
[1,2]<stdout>:#params = 151107538
[1,2]<stdout>:#non emb params = 41066400
[1,1]<stdout>:====================================================================================================
[1,1]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,1]<stdout>:    - dataset : wt103
[1,1]<stdout>:    - n_layer : 16
[1,1]<stdout>:    - n_head : 10
[1,1]<stdout>:    - d_head : 41
[1,1]<stdout>:    - d_embed : 410
[1,1]<stdout>:    - d_model : 410
[1,1]<stdout>:    - d_inner : 2100
[1,1]<stdout>:    - dropout : 0.1
[1,1]<stdout>:    - dropatt : 0.0
[1,1]<stdout>:    - init : normal
[1,1]<stdout>:    - emb_init : normal
[1,1]<stdout>:    - init_range : 0.1
[1,1]<stdout>:    - emb_init_range : 0.01
[1,1]<stdout>:    - init_std : 0.02
[1,1]<stdout>:    - proj_init_std : 0.01
[1,1]<stdout>:    - optim : adam
[1,1]<stdout>:    - lr : 0.00025
[1,1]<stdout>:    - mom : 0.0
[1,1]<stdout>:    - scheduler : cosine
[1,1]<stdout>:    - warmup_step : 0
[1,1]<stdout>:    - decay_rate : 0.5
[1,1]<stdout>:    - lr_min : 0.0
[1,1]<stdout>:    - clip : 0.25
[1,1]<stdout>:    - clip_nonemb : False
[1,1]<stdout>:    - max_step : 5000
[1,1]<stdout>:    - batch_size : 60
[1,1]<stdout>:    - batch_chunk : 1
[1,1]<stdout>:    - tgt_len : 150
[1,1]<stdout>:    - eval_tgt_len : 150
[1,1]<stdout>:    - ext_len : 0
[1,1]<stdout>:    - mem_len : 150
[1,1]<stdout>:    - not_tied : False
[1,1]<stdout>:    - seed : 1111
[1,1]<stdout>:    - cuda : True
[1,1]<stdout>:    - adaptive : True
[1,1]<stdout>:    - div_val : 1
[1,1]<stdout>:    - pre_lnorm : False
[1,1]<stdout>:    - varlen : False
[1,1]<stdout>:    - multi_gpu : False
[1,1]<stdout>:    - log_interval : 25
[1,1]<stdout>:    - eval_interval : 25
[1,1]<stdout>:    - work_dir : LM-TFM-wt103/20230830-005516
[1,1]<stdout>:    - restart : False
[1,1]<stdout>:    - restart_dir : 
[1,1]<stdout>:    - debug : False
[1,1]<stdout>:    - same_length : False
[1,1]<stdout>:    - attn_type : 0
[1,1]<stdout>:    - clamp_len : -1
[1,1]<stdout>:    - eta_min : 0.0
[1,1]<stdout>:    - gpu0_bsz : 4
[1,1]<stdout>:    - max_eval_steps : -1
[1,1]<stdout>:    - sample_softmax : -1
[1,1]<stdout>:    - patience : 0
[1,1]<stdout>:    - finetune_v2 : False
[1,1]<stdout>:    - finetune_v3 : False
[1,1]<stdout>:    - fp16 : False
[1,1]<stdout>:    - static_loss_scale : 1
[1,1]<stdout>:    - dynamic_loss_scale : False
[1,1]<stdout>:    - tied : True
[1,1]<stdout>:    - n_token : 267735
[1,1]<stdout>:    - n_all_param : 151107538
[1,1]<stdout>:    - n_nonemb_param : 41066400
[1,1]<stdout>:====================================================================================================
[1,1]<stdout>:#params = 151107538
[1,1]<stdout>:#non emb params = 41066400
[1,3]<stdout>:====================================================================================================
[1,3]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,3]<stdout>:    - dataset : wt103
[1,3]<stdout>:    - n_layer : 16
[1,3]<stdout>:    - n_head : 10
[1,3]<stdout>:    - d_head : 41
[1,3]<stdout>:    - d_embed : 410
[1,3]<stdout>:    - d_model : 410
[1,3]<stdout>:    - d_inner : 2100
[1,3]<stdout>:    - dropout : 0.1
[1,3]<stdout>:    - dropatt : 0.0
[1,3]<stdout>:    - init : normal
[1,3]<stdout>:    - emb_init : normal
[1,3]<stdout>:    - init_range : 0.1
[1,3]<stdout>:    - emb_init_range : 0.01
[1,3]<stdout>:    - init_std : 0.02
[1,3]<stdout>:    - proj_init_std : 0.01
[1,3]<stdout>:    - optim : adam
[1,3]<stdout>:    - lr : 0.00025
[1,3]<stdout>:    - mom : 0.0
[1,3]<stdout>:    - scheduler : cosine
[1,3]<stdout>:    - warmup_step : 0
[1,3]<stdout>:    - decay_rate : 0.5
[1,3]<stdout>:    - lr_min : 0.0
[1,3]<stdout>:    - clip : 0.25
[1,3]<stdout>:    - clip_nonemb : False
[1,3]<stdout>:    - max_step : 5000
[1,3]<stdout>:    - batch_size : 60
[1,3]<stdout>:    - batch_chunk : 1
[1,3]<stdout>:    - tgt_len : 150
[1,3]<stdout>:    - eval_tgt_len : 150
[1,3]<stdout>:    - ext_len : 0
[1,3]<stdout>:    - mem_len : 150
[1,3]<stdout>:    - not_tied : False
[1,3]<stdout>:    - seed : 1111
[1,3]<stdout>:    - cuda : True
[1,3]<stdout>:    - adaptive : True
[1,3]<stdout>:    - div_val : 1
[1,3]<stdout>:    - pre_lnorm : False
[1,3]<stdout>:    - varlen : False
[1,3]<stdout>:    - multi_gpu : False
[1,3]<stdout>:    - log_interval : 25
[1,3]<stdout>:    - eval_interval : 25
[1,3]<stdout>:    - work_dir : LM-TFM-wt103/20230830-005516
[1,3]<stdout>:    - restart : False
[1,3]<stdout>:    - restart_dir : 
[1,3]<stdout>:    - debug : False
[1,3]<stdout>:    - same_length : False
[1,3]<stdout>:    - attn_type : 0
[1,3]<stdout>:    - clamp_len : -1
[1,3]<stdout>:    - eta_min : 0.0
[1,3]<stdout>:    - gpu0_bsz : 4
[1,3]<stdout>:    - max_eval_steps : -1
[1,3]<stdout>:    - sample_softmax : -1
[1,3]<stdout>:    - patience : 0
[1,3]<stdout>:    - finetune_v2 : False
[1,3]<stdout>:    - finetune_v3 : False
[1,3]<stdout>:    - fp16 : False
[1,3]<stdout>:    - static_loss_scale : 1
[1,3]<stdout>:    - dynamic_loss_scale : False
[1,3]<stdout>:    - tied : True
[1,3]<stdout>:    - n_token : 267735
[1,3]<stdout>:    - n_all_param : 151107538
[1,3]<stdout>:    - n_nonemb_param : 41066400
[1,3]<stdout>:====================================================================================================
[1,3]<stdout>:#params = 151107538
[1,3]<stdout>:#non emb params = 41066400
[1,5]<stdout>:====================================================================================================
[1,5]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,5]<stdout>:    - dataset : wt103
[1,5]<stdout>:    - n_layer : 16
[1,5]<stdout>:    - n_head : 10
[1,5]<stdout>:    - d_head : 41
[1,5]<stdout>:    - d_embed : 410
[1,5]<stdout>:    - d_model : 410
[1,5]<stdout>:    - d_inner : 2100
[1,5]<stdout>:    - dropout : 0.1
[1,5]<stdout>:    - dropatt : 0.0
[1,5]<stdout>:    - init : normal
[1,5]<stdout>:    - emb_init : normal
[1,5]<stdout>:    - init_range : 0.1
[1,5]<stdout>:    - emb_init_range : 0.01
[1,5]<stdout>:    - init_std : 0.02
[1,5]<stdout>:    - proj_init_std : 0.01
[1,5]<stdout>:    - optim : adam
[1,5]<stdout>:    - lr : 0.00025
[1,5]<stdout>:    - mom : 0.0
[1,5]<stdout>:    - scheduler : cosine
[1,5]<stdout>:    - warmup_step : 0
[1,5]<stdout>:    - decay_rate : 0.5
[1,5]<stdout>:    - lr_min : 0.0
[1,5]<stdout>:    - clip : 0.25
[1,5]<stdout>:    - clip_nonemb : False
[1,5]<stdout>:    - max_step : 5000
[1,5]<stdout>:    - batch_size : 60
[1,5]<stdout>:    - batch_chunk : 1
[1,5]<stdout>:    - tgt_len : 150
[1,5]<stdout>:    - eval_tgt_len : 150
[1,5]<stdout>:    - ext_len : 0
[1,5]<stdout>:    - mem_len : 150
[1,5]<stdout>:    - not_tied : False
[1,5]<stdout>:    - seed : 1111
[1,5]<stdout>:    - cuda : True
[1,5]<stdout>:    - adaptive : True
[1,5]<stdout>:    - div_val : 1
[1,5]<stdout>:    - pre_lnorm : False
[1,5]<stdout>:    - varlen : False
[1,5]<stdout>:    - multi_gpu : False
[1,5]<stdout>:    - log_interval : 25
[1,5]<stdout>:    - eval_interval : 25
[1,5]<stdout>:    - work_dir : LM-TFM-wt103/20230829-125516
[1,5]<stdout>:    - restart : False
[1,5]<stdout>:    - restart_dir : 
[1,5]<stdout>:    - debug : False
[1,5]<stdout>:    - same_length : False
[1,5]<stdout>:    - attn_type : 0
[1,5]<stdout>:    - clamp_len : -1
[1,5]<stdout>:    - eta_min : 0.0
[1,5]<stdout>:    - gpu0_bsz : 4
[1,5]<stdout>:    - max_eval_steps : -1
[1,5]<stdout>:    - sample_softmax : -1
[1,5]<stdout>:    - patience : 0
[1,5]<stdout>:    - finetune_v2 : False
[1,5]<stdout>:    - finetune_v3 : False
[1,5]<stdout>:    - fp16 : False
[1,5]<stdout>:    - static_loss_scale : 1
[1,5]<stdout>:    - dynamic_loss_scale : False
[1,5]<stdout>:    - tied : True
[1,5]<stdout>:    - n_token : 267735
[1,5]<stdout>:    - n_all_param : 151107538
[1,5]<stdout>:    - n_nonemb_param : 41066400
[1,5]<stdout>:====================================================================================================
[1,5]<stdout>:#params = 151107538
[1,5]<stdout>:#non emb params = 41066400
[1,7]<stdout>:====================================================================================================
[1,7]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,7]<stdout>:    - dataset : wt103
[1,7]<stdout>:    - n_layer : 16
[1,7]<stdout>:    - n_head : 10
[1,7]<stdout>:    - d_head : 41
[1,7]<stdout>:    - d_embed : 410
[1,7]<stdout>:    - d_model : 410
[1,7]<stdout>:    - d_inner : 2100
[1,7]<stdout>:    - dropout : 0.1
[1,7]<stdout>:    - dropatt : 0.0
[1,7]<stdout>:    - init : normal
[1,7]<stdout>:    - emb_init : normal
[1,7]<stdout>:    - init_range : 0.1
[1,7]<stdout>:    - emb_init_range : 0.01
[1,7]<stdout>:    - init_std : 0.02
[1,7]<stdout>:    - proj_init_std : 0.01
[1,7]<stdout>:    - optim : adam
[1,7]<stdout>:    - lr : 0.00025
[1,7]<stdout>:    - mom : 0.0
[1,7]<stdout>:    - scheduler : cosine
[1,7]<stdout>:    - warmup_step : 0
[1,7]<stdout>:    - decay_rate : 0.5
[1,7]<stdout>:    - lr_min : 0.0
[1,7]<stdout>:    - clip : 0.25
[1,7]<stdout>:    - clip_nonemb : False
[1,7]<stdout>:    - max_step : 5000
[1,7]<stdout>:    - batch_size : 60
[1,7]<stdout>:    - batch_chunk : 1
[1,7]<stdout>:    - tgt_len : 150
[1,7]<stdout>:    - eval_tgt_len : 150
[1,7]<stdout>:    - ext_len : 0
[1,7]<stdout>:    - mem_len : 150
[1,7]<stdout>:    - not_tied : False
[1,7]<stdout>:    - seed : 1111
[1,7]<stdout>:    - cuda : True
[1,7]<stdout>:    - adaptive : True
[1,7]<stdout>:    - div_val : 1
[1,7]<stdout>:    - pre_lnorm : False
[1,7]<stdout>:    - varlen : False
[1,7]<stdout>:    - multi_gpu : False
[1,7]<stdout>:    - log_interval : 25
[1,7]<stdout>:    - eval_interval : 25
[1,7]<stdout>:    - work_dir : LM-TFM-wt103/20230830-005516
[1,7]<stdout>:    - restart : False
[1,7]<stdout>:    - restart_dir : 
[1,7]<stdout>:    - debug : False
[1,7]<stdout>:    - same_length : False
[1,7]<stdout>:    - attn_type : 0
[1,7]<stdout>:    - clamp_len : -1
[1,7]<stdout>:    - eta_min : 0.0
[1,7]<stdout>:    - gpu0_bsz : 4
[1,7]<stdout>:    - max_eval_steps : -1
[1,7]<stdout>:    - sample_softmax : -1
[1,7]<stdout>:    - patience : 0
[1,7]<stdout>:    - finetune_v2 : False
[1,7]<stdout>:    - finetune_v3 : False
[1,7]<stdout>:    - fp16 : False
[1,7]<stdout>:    - static_loss_scale : 1
[1,7]<stdout>:    - dynamic_loss_scale : False
[1,7]<stdout>:    - tied : True
[1,7]<stdout>:    - n_token : 267735
[1,7]<stdout>:    - n_all_param : 151107538
[1,7]<stdout>:    - n_nonemb_param : 41066400
[1,7]<stdout>:====================================================================================================
[1,7]<stdout>:#params = 151107538
[1,7]<stdout>:#non emb params = 41066400
[1,6]<stdout>:====================================================================================================
[1,6]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,6]<stdout>:    - dataset : wt103
[1,6]<stdout>:    - n_layer : 16
[1,6]<stdout>:    - n_head : 10
[1,6]<stdout>:    - d_head : 41
[1,6]<stdout>:    - d_embed : 410
[1,6]<stdout>:    - d_model : 410
[1,6]<stdout>:    - d_inner : 2100
[1,6]<stdout>:    - dropout : 0.1
[1,6]<stdout>:    - dropatt : 0.0
[1,6]<stdout>:    - init : normal
[1,6]<stdout>:    - emb_init : normal
[1,6]<stdout>:    - init_range : 0.1
[1,6]<stdout>:    - emb_init_range : 0.01
[1,6]<stdout>:    - init_std : 0.02
[1,6]<stdout>:    - proj_init_std : 0.01
[1,6]<stdout>:    - optim : adam
[1,6]<stdout>:    - lr : 0.00025
[1,6]<stdout>:    - mom : 0.0
[1,6]<stdout>:    - scheduler : cosine
[1,6]<stdout>:    - warmup_step : 0
[1,6]<stdout>:    - decay_rate : 0.5
[1,6]<stdout>:    - lr_min : 0.0
[1,6]<stdout>:    - clip : 0.25
[1,6]<stdout>:    - clip_nonemb : False
[1,6]<stdout>:    - max_step : 5000
[1,6]<stdout>:    - batch_size : 60
[1,6]<stdout>:    - batch_chunk : 1
[1,6]<stdout>:    - tgt_len : 150
[1,6]<stdout>:    - eval_tgt_len : 150
[1,6]<stdout>:    - ext_len : 0
[1,6]<stdout>:    - mem_len : 150
[1,6]<stdout>:    - not_tied : False
[1,6]<stdout>:    - seed : 1111
[1,6]<stdout>:    - cuda : True
[1,6]<stdout>:    - adaptive : True
[1,6]<stdout>:    - div_val : 1
[1,6]<stdout>:    - pre_lnorm : False
[1,6]<stdout>:    - varlen : False
[1,6]<stdout>:    - multi_gpu : False
[1,6]<stdout>:    - log_interval : 25
[1,6]<stdout>:    - eval_interval : 25
[1,6]<stdout>:    - work_dir : LM-TFM-wt103/20230829-125516
[1,6]<stdout>:    - restart : False
[1,6]<stdout>:    - restart_dir : 
[1,6]<stdout>:    - debug : False
[1,6]<stdout>:    - same_length : False
[1,6]<stdout>:    - attn_type : 0
[1,6]<stdout>:    - clamp_len : -1
[1,6]<stdout>:    - eta_min : 0.0
[1,6]<stdout>:    - gpu0_bsz : 4
[1,6]<stdout>:    - max_eval_steps : -1
[1,6]<stdout>:    - sample_softmax : -1
[1,6]<stdout>:    - patience : 0
[1,6]<stdout>:    - finetune_v2 : False
[1,6]<stdout>:    - finetune_v3 : False
[1,6]<stdout>:    - fp16 : False
[1,6]<stdout>:    - static_loss_scale : 1
[1,6]<stdout>:    - dynamic_loss_scale : False
[1,6]<stdout>:    - tied : True
[1,6]<stdout>:    - n_token : 267735
[1,6]<stdout>:    - n_all_param : 151107538
[1,6]<stdout>:    - n_nonemb_param : 41066400
[1,6]<stdout>:====================================================================================================
[1,6]<stdout>:#params = 151107538
[1,6]<stdout>:#non emb params = 41066400
[1,1]<stdout>:====================================================================================================
[1,1]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,1]<stdout>:    - dataset : wt103
[1,1]<stdout>:    - n_layer : 16
[1,1]<stdout>:    - n_head : 10
[1,1]<stdout>:    - d_head : 41
[1,1]<stdout>:    - d_embed : 410
[1,1]<stdout>:    - d_model : 410
[1,1]<stdout>:    - d_inner : 2100
[1,1]<stdout>:    - dropout : 0.1
[1,1]<stdout>:    - dropatt : 0.0
[1,1]<stdout>:    - init : normal
[1,1]<stdout>:    - emb_init : normal
[1,1]<stdout>:    - init_range : 0.1
[1,1]<stdout>:    - emb_init_range : 0.01
[1,1]<stdout>:    - init_std : 0.02
[1,1]<stdout>:    - proj_init_std : 0.01
[1,1]<stdout>:    - optim : adam
[1,1]<stdout>:    - lr : 0.00025
[1,1]<stdout>:    - mom : 0.0
[1,1]<stdout>:    - scheduler : cosine
[1,1]<stdout>:    - warmup_step : 0
[1,1]<stdout>:    - decay_rate : 0.5
[1,1]<stdout>:    - lr_min : 0.0
[1,1]<stdout>:    - clip : 0.25
[1,1]<stdout>:    - clip_nonemb : False
[1,1]<stdout>:    - max_step : 5000
[1,1]<stdout>:    - batch_size : 60
[1,1]<stdout>:    - batch_chunk : 1
[1,1]<stdout>:    - tgt_len : 150
[1,1]<stdout>:    - eval_tgt_len : 150
[1,1]<stdout>:    - ext_len : 0
[1,1]<stdout>:    - mem_len : 150
[1,1]<stdout>:    - not_tied : False
[1,1]<stdout>:    - seed : 1111
[1,1]<stdout>:    - cuda : True
[1,1]<stdout>:    - adaptive : True
[1,1]<stdout>:    - div_val : 1
[1,1]<stdout>:    - pre_lnorm : False
[1,1]<stdout>:    - varlen : False
[1,1]<stdout>:    - multi_gpu : False
[1,1]<stdout>:    - log_interval : 25
[1,1]<stdout>:    - eval_interval : 25
[1,1]<stdout>:    - work_dir : LM-TFM-wt103/20230830-005516
[1,1]<stdout>:    - restart : False
[1,1]<stdout>:    - restart_dir : 
[1,1]<stdout>:    - debug : False
[1,1]<stdout>:    - same_length : False
[1,1]<stdout>:    - attn_type : 0
[1,1]<stdout>:    - clamp_len : -1
[1,1]<stdout>:    - eta_min : 0.0
[1,1]<stdout>:    - gpu0_bsz : 4
[1,1]<stdout>:    - max_eval_steps : -1
[1,1]<stdout>:    - sample_softmax : -1
[1,1]<stdout>:    - patience : 0
[1,1]<stdout>:    - finetune_v2 : False
[1,1]<stdout>:    - finetune_v3 : False
[1,1]<stdout>:    - fp16 : False
[1,1]<stdout>:    - static_loss_scale : 1
[1,1]<stdout>:    - dynamic_loss_scale : False
[1,1]<stdout>:    - tied : True
[1,1]<stdout>:    - n_token : 267735
[1,1]<stdout>:    - n_all_param : 151107538
[1,1]<stdout>:    - n_nonemb_param : 41066400
[1,1]<stdout>:====================================================================================================
[1,1]<stdout>:#params = 151107538
[1,1]<stdout>:#non emb params = 41066400
[1,7]<stdout>:====================================================================================================
[1,7]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,7]<stdout>:    - dataset : wt103
[1,7]<stdout>:    - n_layer : 16
[1,7]<stdout>:    - n_head : 10
[1,7]<stdout>:    - d_head : 41
[1,7]<stdout>:    - d_embed : 410
[1,7]<stdout>:    - d_model : 410
[1,7]<stdout>:    - d_inner : 2100
[1,7]<stdout>:    - dropout : 0.1
[1,7]<stdout>:    - dropatt : 0.0
[1,7]<stdout>:    - init : normal
[1,7]<stdout>:    - emb_init : normal
[1,7]<stdout>:    - init_range : 0.1
[1,7]<stdout>:    - emb_init_range : 0.01
[1,7]<stdout>:    - init_std : 0.02
[1,7]<stdout>:    - proj_init_std : 0.01
[1,7]<stdout>:    - optim : adam
[1,7]<stdout>:    - lr : 0.00025
[1,3]<stdout>:====================================================================================================
[1,7]<stdout>:    - mom : 0.0
[1,7]<stdout>:    - scheduler : cosine
[1,7]<stdout>:    - warmup_step : 0
[1,7]<stdout>:    - decay_rate : 0.5
[1,7]<stdout>:    - lr_min : 0.0
[1,7]<stdout>:    - clip : 0.25
[1,7]<stdout>:    - clip_nonemb : False
[1,7]<stdout>:    - max_step : 5000
[1,7]<stdout>:    - batch_size : 60
[1,7]<stdout>:    - batch_chunk : 1
[1,7]<stdout>:    - tgt_len : 150
[1,7]<stdout>:    - eval_tgt_len : 150
[1,3]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,3]<stdout>:    - dataset : wt103
[1,3]<stdout>:    - n_layer : 16
[1,7]<stdout>:    - ext_len : 0
[1,7]<stdout>:    - mem_len : 150
[1,3]<stdout>:    - n_head : 10
[1,7]<stdout>:    - not_tied : False
[1,3]<stdout>:    - d_head : 41
[1,3]<stdout>:    - d_embed : 410
[1,7]<stdout>:    - seed : 1111
[1,3]<stdout>:    - d_model : 410
[1,3]<stdout>:    - d_inner : 2100
[1,7]<stdout>:    - cuda : True
[1,3]<stdout>:    - dropout : 0.1
[1,3]<stdout>:    - dropatt : 0.0
[1,7]<stdout>:    - adaptive : True
[1,3]<stdout>:    - init : normal
[1,3]<stdout>:    - emb_init : normal
[1,7]<stdout>:    - div_val : 1
[1,7]<stdout>:    - pre_lnorm : False
[1,7]<stdout>:    - varlen : False
[1,3]<stdout>:    - init_range : 0.1
[1,3]<stdout>:    - emb_init_range : 0.01
[1,3]<stdout>:    - init_std : 0.02
[1,7]<stdout>:    - multi_gpu : False
[1,3]<stdout>:    - proj_init_std : 0.01
[1,3]<stdout>:    - optim : adam
[1,3]<stdout>:    - lr : 0.00025
[1,3]<stdout>:    - mom : 0.0
[1,7]<stdout>:    - log_interval : 25
[1,7]<stdout>:    - eval_interval : 25
[1,3]<stdout>:    - scheduler : cosine
[1,3]<stdout>:    - warmup_step : 0
[1,7]<stdout>:    - work_dir : LM-TFM-wt103/20230830-005516
[1,3]<stdout>:    - decay_rate : 0.5
[1,7]<stdout>:    - restart : False
[1,3]<stdout>:    - lr_min : 0.0
[1,3]<stdout>:    - clip : 0.25
[1,7]<stdout>:    - restart_dir : 
[1,3]<stdout>:    - clip_nonemb : False
[1,3]<stdout>:    - max_step : 5000
[1,7]<stdout>:    - debug : False
[1,7]<stdout>:    - same_length : False
[1,3]<stdout>:    - batch_size : 60
[1,3]<stdout>:    - batch_chunk : 1
[1,3]<stdout>:    - tgt_len : 150
[1,7]<stdout>:    - attn_type : 0
[1,3]<stdout>:    - eval_tgt_len : 150
[1,3]<stdout>:    - ext_len : 0
[1,7]<stdout>:    - clamp_len : -1
[1,3]<stdout>:    - mem_len : 150
[1,3]<stdout>:    - not_tied : False
[1,7]<stdout>:    - eta_min : 0.0
[1,3]<stdout>:    - seed : 1111
[1,3]<stdout>:    - cuda : True
[1,7]<stdout>:    - gpu0_bsz : 4
[1,3]<stdout>:    - adaptive : True
[1,7]<stdout>:    - max_eval_steps : -1
[1,3]<stdout>:    - div_val : 1
[1,3]<stdout>:    - pre_lnorm : False
[1,3]<stdout>:    - varlen : False
[1,3]<stdout>:    - multi_gpu : False
[1,7]<stdout>:    - sample_softmax : -1
[1,7]<stdout>:    - patience : 0
[1,3]<stdout>:    - log_interval : 25
[1,3]<stdout>:    - eval_interval : 25
[1,7]<stdout>:    - finetune_v2 : False
[1,3]<stdout>:    - work_dir : LM-TFM-wt103/20230830-005516
[1,3]<stdout>:    - restart : False
[1,3]<stdout>:    - restart_dir : 
[1,7]<stdout>:    - finetune_v3 : False
[1,3]<stdout>:    - debug : False
[1,3]<stdout>:    - same_length : False
[1,7]<stdout>:    - fp16 : False
[1,7]<stdout>:    - static_loss_scale : 1
[1,3]<stdout>:    - attn_type : 0
[1,3]<stdout>:    - clamp_len : -1
[1,7]<stdout>:    - dynamic_loss_scale : False
[1,3]<stdout>:    - eta_min : 0.0
[1,3]<stdout>:    - gpu0_bsz : 4
[1,3]<stdout>:    - max_eval_steps : -1
[1,7]<stdout>:    - tied : True
[1,7]<stdout>:    - n_token : 267735
[1,3]<stdout>:    - sample_softmax : -1
[1,3]<stdout>:    - patience : 0
[1,7]<stdout>:    - n_all_param : 151107538
[1,3]<stdout>:    - finetune_v2 : False
[1,3]<stdout>:    - finetune_v3 : False
[1,3]<stdout>:    - fp16 : False
[1,7]<stdout>:    - n_nonemb_param : 41066400
[1,3]<stdout>:    - static_loss_scale : 1
[1,3]<stdout>:    - dynamic_loss_scale : False
[1,7]<stdout>:====================================================================================================
[1,3]<stdout>:    - tied : True
[1,7]<stdout>:#params = 151107538
[1,3]<stdout>:    - n_token : 267735
[1,3]<stdout>:    - n_all_param : 151107538
[1,3]<stdout>:    - n_nonemb_param : 41066400
[1,3]<stdout>:====================================================================================================
[1,7]<stdout>:#non emb params = 41066400
[1,3]<stdout>:#params = 151107538
[1,3]<stdout>:#non emb params = 41066400
[1,0]<stdout>:====================================================================================================
[1,0]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,0]<stdout>:    - dataset : wt103
[1,0]<stdout>:    - n_layer : 16
[1,0]<stdout>:    - n_head : 10
[1,0]<stdout>:    - d_head : 41
[1,0]<stdout>:    - d_embed : 410
[1,0]<stdout>:    - d_model : 410
[1,0]<stdout>:    - d_inner : 2100
[1,0]<stdout>:    - dropout : 0.1
[1,0]<stdout>:    - dropatt : 0.0
[1,0]<stdout>:    - init : normal
[1,0]<stdout>:    - emb_init : normal
[1,0]<stdout>:    - init_range : 0.1
[1,0]<stdout>:    - emb_init_range : 0.01
[1,0]<stdout>:    - init_std : 0.02
[1,0]<stdout>:    - proj_init_std : 0.01
[1,0]<stdout>:    - optim : adam
[1,0]<stdout>:    - lr : 0.00025
[1,0]<stdout>:    - mom : 0.0
[1,0]<stdout>:    - scheduler : cosine
[1,0]<stdout>:    - warmup_step : 0
[1,0]<stdout>:    - decay_rate : 0.5
[1,0]<stdout>:    - lr_min : 0.0
[1,0]<stdout>:    - clip : 0.25
[1,0]<stdout>:    - clip_nonemb : False
[1,0]<stdout>:    - max_step : 5000
[1,0]<stdout>:    - batch_size : 60
[1,0]<stdout>:    - batch_chunk : 1
[1,0]<stdout>:    - tgt_len : 150
[1,0]<stdout>:    - eval_tgt_len : 150
[1,0]<stdout>:    - ext_len : 0
[1,0]<stdout>:    - mem_len : 150
[1,0]<stdout>:    - not_tied : False
[1,0]<stdout>:    - seed : 1111
[1,0]<stdout>:    - cuda : True
[1,0]<stdout>:    - adaptive : True
[1,0]<stdout>:    - div_val : 1
[1,0]<stdout>:    - pre_lnorm : False
[1,0]<stdout>:    - varlen : False
[1,0]<stdout>:    - multi_gpu : False
[1,0]<stdout>:    - log_interval : 25
[1,0]<stdout>:    - eval_interval : 25
[1,0]<stdout>:    - work_dir : LM-TFM-wt103/20230829-125516
[1,0]<stdout>:    - restart : False
[1,0]<stdout>:    - restart_dir : 
[1,0]<stdout>:    - debug : False
[1,0]<stdout>:    - same_length : False
[1,0]<stdout>:    - attn_type : 0
[1,0]<stdout>:    - clamp_len : -1
[1,0]<stdout>:    - eta_min : 0.0
[1,0]<stdout>:    - gpu0_bsz : 4
[1,0]<stdout>:    - max_eval_steps : -1
[1,0]<stdout>:    - sample_softmax : -1
[1,0]<stdout>:    - patience : 0
[1,0]<stdout>:    - finetune_v2 : False
[1,0]<stdout>:    - finetune_v3 : False
[1,0]<stdout>:    - fp16 : False
[1,0]<stdout>:    - static_loss_scale : 1
[1,0]<stdout>:    - dynamic_loss_scale : False
[1,0]<stdout>:    - tied : True
[1,0]<stdout>:    - n_token : 267735
[1,0]<stdout>:    - n_all_param : 151107538
[1,0]<stdout>:    - n_nonemb_param : 41066400
[1,0]<stdout>:====================================================================================================
[1,0]<stdout>:#params = 151107538
[1,0]<stdout>:#non emb params = 41066400
[1,0]<stdout>:====================================================================================================
[1,0]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,0]<stdout>:    - dataset : wt103
[1,0]<stdout>:    - n_layer : 16
[1,0]<stdout>:    - n_head : 10
[1,0]<stdout>:    - d_head : 41
[1,0]<stdout>:    - d_embed : 410
[1,0]<stdout>:    - d_model : 410
[1,0]<stdout>:    - d_inner : 2100
[1,0]<stdout>:    - dropout : 0.1
[1,0]<stdout>:    - dropatt : 0.0
[1,0]<stdout>:    - init : normal
[1,0]<stdout>:    - emb_init : normal
[1,0]<stdout>:    - init_range : 0.1
[1,0]<stdout>:    - emb_init_range : 0.01
[1,0]<stdout>:    - init_std : 0.02
[1,0]<stdout>:    - proj_init_std : 0.01
[1,0]<stdout>:    - optim : adam
[1,0]<stdout>:    - lr : 0.00025
[1,0]<stdout>:    - mom : 0.0
[1,0]<stdout>:    - scheduler : cosine
[1,0]<stdout>:    - warmup_step : 0
[1,0]<stdout>:    - decay_rate : 0.5
[1,0]<stdout>:    - lr_min : 0.0
[1,0]<stdout>:    - clip : 0.25
[1,0]<stdout>:    - clip_nonemb : False
[1,0]<stdout>:    - max_step : 5000
[1,0]<stdout>:    - batch_size : 60
[1,0]<stdout>:    - batch_chunk : 1
[1,0]<stdout>:    - tgt_len : 150
[1,0]<stdout>:    - eval_tgt_len : 150
[1,0]<stdout>:    - ext_len : 0
[1,0]<stdout>:    - mem_len : 150
[1,0]<stdout>:    - not_tied : False
[1,0]<stdout>:    - seed : 1111
[1,0]<stdout>:    - cuda : True
[1,0]<stdout>:    - adaptive : True
[1,0]<stdout>:    - div_val : 1
[1,0]<stdout>:    - pre_lnorm : False
[1,0]<stdout>:    - varlen : False
[1,0]<stdout>:    - multi_gpu : False
[1,0]<stdout>:    - log_interval : 25
[1,0]<stdout>:    - eval_interval : 25
[1,0]<stdout>:    - work_dir : LM-TFM-wt103/20230829-125516
[1,0]<stdout>:    - restart : False
[1,0]<stdout>:    - restart_dir : 
[1,0]<stdout>:    - debug : False
[1,0]<stdout>:    - same_length : False
[1,0]<stdout>:    - attn_type : 0
[1,0]<stdout>:    - clamp_len : -1
[1,0]<stdout>:    - eta_min : 0.0
[1,0]<stdout>:    - gpu0_bsz : 4
[1,0]<stdout>:    - max_eval_steps : -1
[1,0]<stdout>:    - sample_softmax : -1
[1,0]<stdout>:    - patience : 0
[1,0]<stdout>:    - finetune_v2 : False
[1,0]<stdout>:    - finetune_v3 : False
[1,0]<stdout>:    - fp16 : False
[1,0]<stdout>:    - static_loss_scale : 1
[1,0]<stdout>:    - dynamic_loss_scale : False
[1,0]<stdout>:    - tied : True
[1,0]<stdout>:    - n_token : 267735
[1,0]<stdout>:    - n_all_param : 151107538
[1,0]<stdout>:    - n_nonemb_param : 41066400
[1,0]<stdout>:====================================================================================================
[1,0]<stdout>:#params = 151107538
[1,0]<stdout>:#non emb params = 41066400
[1,2]<stdout>:====================================================================================================
[1,2]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,2]<stdout>:    - dataset : wt103
[1,2]<stdout>:    - n_layer : 16
[1,2]<stdout>:    - n_head : 10
[1,2]<stdout>:    - d_head : 41
[1,2]<stdout>:    - d_embed : 410
[1,2]<stdout>:    - d_model : 410
[1,2]<stdout>:    - d_inner : 2100
[1,2]<stdout>:    - dropout : 0.1
[1,2]<stdout>:    - dropatt : 0.0
[1,2]<stdout>:    - init : normal
[1,2]<stdout>:    - emb_init : normal
[1,2]<stdout>:    - init_range : 0.1
[1,2]<stdout>:    - emb_init_range : 0.01
[1,2]<stdout>:    - init_std : 0.02
[1,2]<stdout>:    - proj_init_std : 0.01
[1,2]<stdout>:    - optim : adam
[1,2]<stdout>:    - lr : 0.00025
[1,2]<stdout>:    - mom : 0.0
[1,2]<stdout>:    - scheduler : cosine
[1,2]<stdout>:    - warmup_step : 0
[1,2]<stdout>:    - decay_rate : 0.5
[1,2]<stdout>:    - lr_min : 0.0
[1,2]<stdout>:    - clip : 0.25
[1,2]<stdout>:    - clip_nonemb : False
[1,2]<stdout>:    - max_step : 5000
[1,2]<stdout>:    - batch_size : 60
[1,2]<stdout>:    - batch_chunk : 1
[1,2]<stdout>:    - tgt_len : 150
[1,2]<stdout>:    - eval_tgt_len : 150
[1,2]<stdout>:    - ext_len : 0
[1,2]<stdout>:    - mem_len : 150
[1,2]<stdout>:    - not_tied : False
[1,2]<stdout>:    - seed : 1111
[1,2]<stdout>:    - cuda : True
[1,2]<stdout>:    - adaptive : True
[1,2]<stdout>:    - div_val : 1
[1,2]<stdout>:    - pre_lnorm : False
[1,2]<stdout>:    - varlen : False
[1,2]<stdout>:    - multi_gpu : False
[1,2]<stdout>:    - log_interval : 25
[1,2]<stdout>:    - eval_interval : 25
[1,2]<stdout>:    - work_dir : LM-TFM-wt103/20230829-125516
[1,2]<stdout>:    - restart : False
[1,2]<stdout>:    - restart_dir : 
[1,2]<stdout>:    - debug : False
[1,2]<stdout>:    - same_length : False
[1,2]<stdout>:    - attn_type : 0
[1,2]<stdout>:    - clamp_len : -1
[1,2]<stdout>:    - eta_min : 0.0
[1,2]<stdout>:    - gpu0_bsz : 4
[1,2]<stdout>:    - max_eval_steps : -1
[1,2]<stdout>:    - sample_softmax : -1
[1,2]<stdout>:    - patience : 0
[1,2]<stdout>:    - finetune_v2 : False
[1,2]<stdout>:    - finetune_v3 : False
[1,2]<stdout>:    - fp16 : False
[1,2]<stdout>:    - static_loss_scale : 1
[1,2]<stdout>:    - dynamic_loss_scale : False
[1,2]<stdout>:    - tied : True
[1,2]<stdout>:    - n_token : 267735
[1,2]<stdout>:    - n_all_param : 151107538
[1,2]<stdout>:    - n_nonemb_param : 41066400
[1,2]<stdout>:====================================================================================================
[1,2]<stdout>:#params = 151107538
[1,2]<stdout>:#non emb params = 41066400
[1,4]<stdout>:====================================================================================================
[1,4]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,4]<stdout>:    - dataset : wt103
[1,4]<stdout>:    - n_layer : 16
[1,4]<stdout>:    - n_head : 10
[1,4]<stdout>:    - d_head : 41
[1,4]<stdout>:    - d_embed : 410
[1,4]<stdout>:    - d_model : 410
[1,4]<stdout>:    - d_inner : 2100
[1,4]<stdout>:    - dropout : 0.1
[1,4]<stdout>:    - dropatt : 0.0
[1,4]<stdout>:    - init : normal
[1,4]<stdout>:    - emb_init : normal
[1,4]<stdout>:    - init_range : 0.1
[1,4]<stdout>:    - emb_init_range : 0.01
[1,4]<stdout>:    - init_std : 0.02
[1,4]<stdout>:    - proj_init_std : 0.01
[1,4]<stdout>:    - optim : adam
[1,4]<stdout>:    - lr : 0.00025
[1,4]<stdout>:    - mom : 0.0
[1,4]<stdout>:    - scheduler : cosine
[1,4]<stdout>:    - warmup_step : 0
[1,4]<stdout>:    - decay_rate : 0.5
[1,4]<stdout>:    - lr_min : 0.0
[1,4]<stdout>:    - clip : 0.25
[1,4]<stdout>:    - clip_nonemb : False
[1,4]<stdout>:    - max_step : 5000
[1,4]<stdout>:    - batch_size : 60
[1,4]<stdout>:    - batch_chunk : 1
[1,4]<stdout>:    - tgt_len : 150
[1,4]<stdout>:    - eval_tgt_len : 150
[1,4]<stdout>:    - ext_len : 0
[1,4]<stdout>:    - mem_len : 150
[1,4]<stdout>:    - not_tied : False
[1,4]<stdout>:    - seed : 1111
[1,4]<stdout>:    - cuda : True
[1,4]<stdout>:    - adaptive : True
[1,4]<stdout>:    - div_val : 1
[1,4]<stdout>:    - pre_lnorm : False
[1,4]<stdout>:    - varlen : False
[1,4]<stdout>:    - multi_gpu : False
[1,4]<stdout>:    - log_interval : 25
[1,4]<stdout>:    - eval_interval : 25
[1,4]<stdout>:    - work_dir : LM-TFM-wt103/20230830-005516
[1,4]<stdout>:    - restart : False
[1,4]<stdout>:    - restart_dir : 
[1,4]<stdout>:    - debug : False
[1,4]<stdout>:    - same_length : False
[1,4]<stdout>:    - attn_type : 0
[1,4]<stdout>:    - clamp_len : -1
[1,4]<stdout>:    - eta_min : 0.0
[1,4]<stdout>:    - gpu0_bsz : 4
[1,4]<stdout>:    - max_eval_steps : -1
[1,4]<stdout>:    - sample_softmax : -1
[1,4]<stdout>:    - patience : 0
[1,4]<stdout>:    - finetune_v2 : False
[1,4]<stdout>:    - finetune_v3 : False
[1,4]<stdout>:    - fp16 : False
[1,4]<stdout>:    - static_loss_scale : 1
[1,4]<stdout>:    - dynamic_loss_scale : False
[1,4]<stdout>:    - tied : True
[1,4]<stdout>:    - n_token : 267735
[1,4]<stdout>:    - n_all_param : 151107538
[1,4]<stdout>:    - n_nonemb_param : 41066400
[1,4]<stdout>:====================================================================================================
[1,4]<stdout>:#params = 151107538
[1,4]<stdout>:#non emb params = 41066400
[1,6]<stdout>:====================================================================================================
[1,6]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,6]<stdout>:    - dataset : wt103
[1,6]<stdout>:    - n_layer : 16
[1,6]<stdout>:    - n_head : 10
[1,6]<stdout>:    - d_head : 41
[1,6]<stdout>:    - d_embed : 410
[1,6]<stdout>:    - d_model : 410
[1,6]<stdout>:    - d_inner : 2100
[1,6]<stdout>:    - dropout : 0.1
[1,6]<stdout>:    - dropatt : 0.0
[1,6]<stdout>:    - init : normal
[1,6]<stdout>:    - emb_init : normal
[1,6]<stdout>:    - init_range : 0.1
[1,6]<stdout>:    - emb_init_range : 0.01
[1,6]<stdout>:    - init_std : 0.02
[1,6]<stdout>:    - proj_init_std : 0.01
[1,6]<stdout>:    - optim : adam
[1,6]<stdout>:    - lr : 0.00025
[1,6]<stdout>:    - mom : 0.0
[1,6]<stdout>:    - scheduler : cosine
[1,6]<stdout>:    - warmup_step : 0
[1,6]<stdout>:    - decay_rate : 0.5
[1,6]<stdout>:    - lr_min : 0.0
[1,6]<stdout>:    - clip : 0.25
[1,6]<stdout>:    - clip_nonemb : False
[1,6]<stdout>:    - max_step : 5000
[1,6]<stdout>:    - batch_size : 60
[1,6]<stdout>:    - batch_chunk : 1
[1,6]<stdout>:    - tgt_len : 150
[1,6]<stdout>:    - eval_tgt_len : 150
[1,6]<stdout>:    - ext_len : 0
[1,6]<stdout>:    - mem_len : 150
[1,6]<stdout>:    - not_tied : False
[1,6]<stdout>:    - seed : 1111
[1,6]<stdout>:    - cuda : True
[1,6]<stdout>:    - adaptive : True
[1,6]<stdout>:    - div_val : 1
[1,6]<stdout>:    - pre_lnorm : False
[1,6]<stdout>:    - varlen : False
[1,6]<stdout>:    - multi_gpu : False
[1,6]<stdout>:    - log_interval : 25
[1,6]<stdout>:    - eval_interval : 25
[1,6]<stdout>:    - work_dir : LM-TFM-wt103/20230829-125516
[1,6]<stdout>:    - restart : False
[1,6]<stdout>:    - restart_dir : 
[1,6]<stdout>:    - debug : False
[1,6]<stdout>:    - same_length : False
[1,6]<stdout>:    - attn_type : 0
[1,6]<stdout>:    - clamp_len : -1
[1,6]<stdout>:    - eta_min : 0.0
[1,6]<stdout>:    - gpu0_bsz : 4
[1,6]<stdout>:    - max_eval_steps : -1
[1,6]<stdout>:    - sample_softmax : -1
[1,6]<stdout>:    - patience : 0
[1,6]<stdout>:    - finetune_v2 : False
[1,6]<stdout>:    - finetune_v3 : False
[1,6]<stdout>:    - fp16 : False
[1,6]<stdout>:    - static_loss_scale : 1
[1,6]<stdout>:    - dynamic_loss_scale : False
[1,6]<stdout>:    - tied : True
[1,6]<stdout>:    - n_token : 267735
[1,6]<stdout>:    - n_all_param : 151107538
[1,6]<stdout>:    - n_nonemb_param : 41066400
[1,6]<stdout>:====================================================================================================
[1,6]<stdout>:#params = 151107538
[1,6]<stdout>:#non emb params = 41066400
[1,4]<stdout>:====================================================================================================
[1,4]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,4]<stdout>:    - dataset : wt103
[1,4]<stdout>:    - n_layer : 16
[1,4]<stdout>:    - n_head : 10
[1,4]<stdout>:    - d_head : 41
[1,4]<stdout>:    - d_embed : 410
[1,4]<stdout>:    - d_model : 410
[1,4]<stdout>:    - d_inner : 2100
[1,4]<stdout>:    - dropout : 0.1
[1,4]<stdout>:    - dropatt : 0.0
[1,4]<stdout>:    - init : normal
[1,4]<stdout>:    - emb_init : normal
[1,4]<stdout>:    - init_range : 0.1
[1,4]<stdout>:    - emb_init_range : 0.01
[1,4]<stdout>:    - init_std : 0.02
[1,4]<stdout>:    - proj_init_std : 0.01
[1,4]<stdout>:    - optim : adam
[1,4]<stdout>:    - lr : 0.00025
[1,4]<stdout>:    - mom : 0.0
[1,4]<stdout>:    - scheduler : cosine
[1,4]<stdout>:    - warmup_step : 0
[1,4]<stdout>:    - decay_rate : 0.5
[1,4]<stdout>:    - lr_min : 0.0
[1,4]<stdout>:    - clip : 0.25
[1,4]<stdout>:    - clip_nonemb : False
[1,4]<stdout>:    - max_step : 5000
[1,4]<stdout>:    - batch_size : 60
[1,4]<stdout>:    - batch_chunk : 1
[1,4]<stdout>:    - tgt_len : 150
[1,4]<stdout>:    - eval_tgt_len : 150
[1,4]<stdout>:    - ext_len : 0
[1,4]<stdout>:    - mem_len : 150
[1,4]<stdout>:    - not_tied : False
[1,4]<stdout>:    - seed : 1111
[1,4]<stdout>:    - cuda : True
[1,4]<stdout>:    - adaptive : True
[1,4]<stdout>:    - div_val : 1
[1,4]<stdout>:    - pre_lnorm : False
[1,4]<stdout>:    - varlen : False
[1,4]<stdout>:    - multi_gpu : False
[1,4]<stdout>:    - log_interval : 25
[1,4]<stdout>:    - eval_interval : 25
[1,4]<stdout>:    - work_dir : LM-TFM-wt103/20230830-005516
[1,4]<stdout>:    - restart : False
[1,4]<stdout>:    - restart_dir : 
[1,4]<stdout>:    - debug : False
[1,4]<stdout>:    - same_length : False
[1,4]<stdout>:    - attn_type : 0
[1,4]<stdout>:    - clamp_len : -1
[1,4]<stdout>:    - eta_min : 0.0
[1,4]<stdout>:    - gpu0_bsz : 4
[1,4]<stdout>:    - max_eval_steps : -1
[1,4]<stdout>:    - sample_softmax : -1
[1,4]<stdout>:    - patience : 0
[1,4]<stdout>:    - finetune_v2 : False
[1,4]<stdout>:    - finetune_v3 : False
[1,4]<stdout>:    - fp16 : False
[1,4]<stdout>:    - static_loss_scale : 1
[1,4]<stdout>:    - dynamic_loss_scale : False
[1,4]<stdout>:    - tied : True
[1,4]<stdout>:    - n_token : 267735
[1,4]<stdout>:    - n_all_param : 151107538
[1,4]<stdout>:    - n_nonemb_param : 41066400
[1,4]<stdout>:====================================================================================================
[1,4]<stdout>:#params = 151107538
[1,4]<stdout>:#non emb params = 41066400
[1,2]<stdout>:====================================================================================================
[1,2]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,2]<stdout>:    - dataset : wt103
[1,2]<stdout>:    - n_layer : 16
[1,2]<stdout>:    - n_head : 10
[1,2]<stdout>:    - d_head : 41
[1,2]<stdout>:    - d_embed : 410
[1,2]<stdout>:    - d_model : 410
[1,2]<stdout>:    - d_inner : 2100
[1,2]<stdout>:    - dropout : 0.1
[1,2]<stdout>:    - dropatt : 0.0
[1,2]<stdout>:    - init : normal
[1,2]<stdout>:    - emb_init : normal
[1,2]<stdout>:    - init_range : 0.1
[1,2]<stdout>:    - emb_init_range : 0.01
[1,2]<stdout>:    - init_std : 0.02
[1,2]<stdout>:    - proj_init_std : 0.01
[1,2]<stdout>:    - optim : adam
[1,2]<stdout>:    - lr : 0.00025
[1,2]<stdout>:    - mom : 0.0
[1,2]<stdout>:    - scheduler : cosine
[1,2]<stdout>:    - warmup_step : 0
[1,2]<stdout>:    - decay_rate : 0.5
[1,2]<stdout>:    - lr_min : 0.0
[1,2]<stdout>:    - clip : 0.25
[1,2]<stdout>:    - clip_nonemb : False
[1,2]<stdout>:    - max_step : 5000
[1,2]<stdout>:    - batch_size : 60
[1,2]<stdout>:    - batch_chunk : 1
[1,2]<stdout>:    - tgt_len : 150
[1,2]<stdout>:    - eval_tgt_len : 150
[1,2]<stdout>:    - ext_len : 0
[1,2]<stdout>:    - mem_len : 150
[1,2]<stdout>:    - not_tied : False
[1,2]<stdout>:    - seed : 1111
[1,2]<stdout>:    - cuda : True
[1,2]<stdout>:    - adaptive : True
[1,2]<stdout>:    - div_val : 1
[1,2]<stdout>:    - pre_lnorm : False
[1,2]<stdout>:    - varlen : False
[1,2]<stdout>:    - multi_gpu : False
[1,2]<stdout>:    - log_interval : 25
[1,2]<stdout>:    - eval_interval : 25
[1,2]<stdout>:    - work_dir : LM-TFM-wt103/20230829-125516
[1,2]<stdout>:    - restart : False
[1,2]<stdout>:    - restart_dir : 
[1,2]<stdout>:    - debug : False
[1,2]<stdout>:    - same_length : False
[1,2]<stdout>:    - attn_type : 0
[1,2]<stdout>:    - clamp_len : -1
[1,2]<stdout>:    - eta_min : 0.0
[1,2]<stdout>:    - gpu0_bsz : 4
[1,2]<stdout>:    - max_eval_steps : -1
[1,2]<stdout>:    - sample_softmax : -1
[1,2]<stdout>:    - patience : 0
[1,2]<stdout>:    - finetune_v2 : False
[1,2]<stdout>:    - finetune_v3 : False
[1,2]<stdout>:    - fp16 : False
[1,2]<stdout>:    - static_loss_scale : 1
[1,2]<stdout>:    - dynamic_loss_scale : False
[1,2]<stdout>:    - tied : True
[1,2]<stdout>:    - n_token : 267735
[1,2]<stdout>:    - n_all_param : 151107538
[1,2]<stdout>:    - n_nonemb_param : 41066400
[1,2]<stdout>:====================================================================================================
[1,2]<stdout>:#params = 151107538
[1,2]<stdout>:#non emb params = 41066400
[1,1]<stdout>:====================================================================================================
[1,1]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,1]<stdout>:    - dataset : wt103
[1,1]<stdout>:    - n_layer : 16
[1,1]<stdout>:    - n_head : 10
[1,1]<stdout>:    - d_head : 41
[1,1]<stdout>:    - d_embed : 410
[1,1]<stdout>:    - d_model : 410
[1,1]<stdout>:    - d_inner : 2100
[1,1]<stdout>:    - dropout : 0.1
[1,1]<stdout>:    - dropatt : 0.0
[1,1]<stdout>:    - init : normal
[1,1]<stdout>:    - emb_init : normal
[1,1]<stdout>:    - init_range : 0.1
[1,1]<stdout>:    - emb_init_range : 0.01
[1,1]<stdout>:    - init_std : 0.02
[1,1]<stdout>:    - proj_init_std : 0.01
[1,1]<stdout>:    - optim : adam
[1,1]<stdout>:    - lr : 0.00025
[1,1]<stdout>:    - mom : 0.0
[1,1]<stdout>:    - scheduler : cosine
[1,1]<stdout>:    - warmup_step : 0
[1,1]<stdout>:    - decay_rate : 0.5
[1,1]<stdout>:    - lr_min : 0.0
[1,1]<stdout>:    - clip : 0.25
[1,1]<stdout>:    - clip_nonemb : False
[1,1]<stdout>:    - max_step : 5000
[1,1]<stdout>:    - batch_size : 60
[1,1]<stdout>:    - batch_chunk : 1
[1,1]<stdout>:    - tgt_len : 150
[1,1]<stdout>:    - eval_tgt_len : 150
[1,1]<stdout>:    - ext_len : 0
[1,1]<stdout>:    - mem_len : 150
[1,1]<stdout>:    - not_tied : False
[1,1]<stdout>:    - seed : 1111
[1,1]<stdout>:    - cuda : True
[1,1]<stdout>:    - adaptive : True
[1,1]<stdout>:    - div_val : 1
[1,1]<stdout>:    - pre_lnorm : False
[1,1]<stdout>:    - varlen : False
[1,1]<stdout>:    - multi_gpu : False
[1,1]<stdout>:    - log_interval : 25
[1,1]<stdout>:    - eval_interval : 25
[1,1]<stdout>:    - work_dir : LM-TFM-wt103/20230830-005516
[1,1]<stdout>:    - restart : False
[1,1]<stdout>:    - restart_dir : 
[1,1]<stdout>:    - debug : False
[1,1]<stdout>:    - same_length : False
[1,1]<stdout>:    - attn_type : 0
[1,1]<stdout>:    - clamp_len : -1
[1,1]<stdout>:    - eta_min : 0.0
[1,1]<stdout>:    - gpu0_bsz : 4
[1,1]<stdout>:    - max_eval_steps : -1
[1,1]<stdout>:    - sample_softmax : -1
[1,1]<stdout>:    - patience : 0
[1,1]<stdout>:    - finetune_v2 : False
[1,1]<stdout>:    - finetune_v3 : False
[1,1]<stdout>:    - fp16 : False
[1,1]<stdout>:    - static_loss_scale : 1
[1,1]<stdout>:    - dynamic_loss_scale : False
[1,1]<stdout>:    - tied : True
[1,1]<stdout>:    - n_token : 267735
[1,1]<stdout>:    - n_all_param : 151107538
[1,1]<stdout>:    - n_nonemb_param : 41066400
[1,1]<stdout>:====================================================================================================
[1,1]<stdout>:#params = 151107538
[1,1]<stdout>:#non emb params = 41066400
[1,3]<stdout>:====================================================================================================
[1,3]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,3]<stdout>:    - dataset : wt103
[1,3]<stdout>:    - n_layer : 16
[1,3]<stdout>:    - n_head : 10
[1,3]<stdout>:    - d_head : 41
[1,3]<stdout>:    - d_embed : 410
[1,3]<stdout>:    - d_model : 410
[1,3]<stdout>:    - d_inner : 2100
[1,3]<stdout>:    - dropout : 0.1
[1,3]<stdout>:    - dropatt : 0.0
[1,3]<stdout>:    - init : normal
[1,3]<stdout>:    - emb_init : normal
[1,3]<stdout>:    - init_range : 0.1
[1,3]<stdout>:    - emb_init_range : 0.01
[1,3]<stdout>:    - init_std : 0.02
[1,3]<stdout>:    - proj_init_std : 0.01
[1,3]<stdout>:    - optim : adam
[1,3]<stdout>:    - lr : 0.00025
[1,3]<stdout>:    - mom : 0.0
[1,3]<stdout>:    - scheduler : cosine
[1,3]<stdout>:    - warmup_step : 0
[1,3]<stdout>:    - decay_rate : 0.5
[1,3]<stdout>:    - lr_min : 0.0
[1,3]<stdout>:    - clip : 0.25
[1,3]<stdout>:    - clip_nonemb : False
[1,3]<stdout>:    - max_step : 5000
[1,3]<stdout>:    - batch_size : 60
[1,3]<stdout>:    - batch_chunk : 1
[1,3]<stdout>:    - tgt_len : 150
[1,3]<stdout>:    - eval_tgt_len : 150
[1,3]<stdout>:    - ext_len : 0
[1,3]<stdout>:    - mem_len : 150
[1,3]<stdout>:    - not_tied : False
[1,3]<stdout>:    - seed : 1111
[1,3]<stdout>:    - cuda : True
[1,3]<stdout>:    - adaptive : True
[1,3]<stdout>:    - div_val : 1
[1,3]<stdout>:    - pre_lnorm : False
[1,3]<stdout>:    - varlen : False
[1,3]<stdout>:    - multi_gpu : False
[1,3]<stdout>:    - log_interval : 25
[1,3]<stdout>:    - eval_interval : 25
[1,3]<stdout>:    - work_dir : LM-TFM-wt103/20230830-005516
[1,3]<stdout>:    - restart : False
[1,3]<stdout>:    - restart_dir : 
[1,3]<stdout>:    - debug : False
[1,3]<stdout>:    - same_length : False
[1,3]<stdout>:    - attn_type : 0
[1,3]<stdout>:    - clamp_len : -1
[1,3]<stdout>:    - eta_min : 0.0
[1,3]<stdout>:    - gpu0_bsz : 4
[1,3]<stdout>:    - max_eval_steps : -1
[1,3]<stdout>:    - sample_softmax : -1
[1,3]<stdout>:    - patience : 0
[1,3]<stdout>:    - finetune_v2 : False
[1,3]<stdout>:    - finetune_v3 : False
[1,3]<stdout>:    - fp16 : False
[1,3]<stdout>:    - static_loss_scale : 1
[1,3]<stdout>:    - dynamic_loss_scale : False
[1,3]<stdout>:    - tied : True
[1,3]<stdout>:    - n_token : 267735
[1,3]<stdout>:    - n_all_param : 151107538
[1,3]<stdout>:    - n_nonemb_param : 41066400
[1,3]<stdout>:====================================================================================================
[1,3]<stdout>:#params = 151107538
[1,3]<stdout>:#non emb params = 41066400
[1,4]<stderr>:Traceback (most recent call last):
[1,4]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_actopk.py", line 418, in <module>
[1,5]<stdout>:====================================================================================================
[1,5]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,5]<stdout>:    - dataset : wt103
[1,5]<stdout>:    - n_layer : 16
[1,5]<stdout>:    - n_head : 10
[1,5]<stdout>:    - d_head : 41
[1,5]<stdout>:    - d_embed : 410
[1,5]<stdout>:    - d_model : 410
[1,5]<stdout>:    - d_inner : 2100
[1,5]<stdout>:    - dropout : 0.1
[1,5]<stdout>:    - dropatt : 0.0
[1,5]<stdout>:    - init : normal
[1,5]<stdout>:    - emb_init : normal
[1,5]<stdout>:    - init_range : 0.1
[1,5]<stdout>:    - emb_init_range : 0.01
[1,5]<stdout>:    - init_std : 0.02
[1,5]<stdout>:    - proj_init_std : 0.01
[1,5]<stdout>:    - optim : adam
[1,5]<stdout>:    - lr : 0.00025
[1,5]<stdout>:    - mom : 0.0
[1,5]<stdout>:    - scheduler : cosine
[1,5]<stdout>:    - warmup_step : 0
[1,5]<stdout>:    - decay_rate : 0.5
[1,5]<stdout>:    - lr_min : 0.0
[1,5]<stdout>:    - clip : 0.25
[1,5]<stdout>:    - clip_nonemb : False
[1,5]<stdout>:    - max_step : 5000
[1,5]<stdout>:    - batch_size : 60
[1,5]<stdout>:    - batch_chunk : 1
[1,5]<stdout>:    - tgt_len : 150
[1,5]<stdout>:    - eval_tgt_len : 150
[1,5]<stdout>:    - ext_len : 0
[1,5]<stdout>:    - mem_len : 150
[1,5]<stdout>:    - not_tied : False
[1,5]<stdout>:    - seed : 1111
[1,5]<stdout>:    - cuda : True
[1,5]<stdout>:    - adaptive : True
[1,5]<stdout>:    - div_val : 1
[1,5]<stdout>:    - pre_lnorm : False
[1,5]<stdout>:    - varlen : False
[1,5]<stdout>:    - multi_gpu : False
[1,5]<stdout>:    - log_interval : 25
[1,5]<stdout>:    - eval_interval : 25
[1,5]<stdout>:    - work_dir : LM-TFM-wt103/20230829-125516
[1,5]<stdout>:    - restart : False
[1,5]<stdout>:    - restart_dir : 
[1,5]<stdout>:    - debug : False
[1,5]<stdout>:    - same_length : False
[1,5]<stdout>:    - attn_type : 0
[1,5]<stdout>:    - clamp_len : -1
[1,5]<stdout>:    - eta_min : 0.0
[1,5]<stdout>:    - gpu0_bsz : 4
[1,5]<stdout>:    - max_eval_steps : -1
[1,5]<stdout>:    - sample_softmax : -1
[1,5]<stdout>:    - patience : 0
[1,5]<stdout>:    - finetune_v2 : False
[1,5]<stdout>:    - finetune_v3 : False
[1,5]<stdout>:    - fp16 : False
[1,5]<stdout>:    - static_loss_scale : 1
[1,5]<stdout>:    - dynamic_loss_scale : False
[1,5]<stdout>:    - tied : True
[1,5]<stdout>:    - n_token : 267735
[1,5]<stdout>:    - n_all_param : 151107538
[1,5]<stdout>:    - n_nonemb_param : 41066400
[1,5]<stdout>:====================================================================================================
[1,5]<stdout>:#params = 151107538
[1,5]<stdout>:#non emb params = 41066400
[1,7]<stdout>:====================================================================================================
[1,7]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,7]<stdout>:    - dataset : wt103
[1,7]<stdout>:    - n_layer : 16
[1,7]<stdout>:    - n_head : 10
[1,7]<stdout>:    - d_head : 41
[1,7]<stdout>:    - d_embed : 410
[1,7]<stdout>:    - d_model : 410
[1,7]<stdout>:    - d_inner : 2100
[1,7]<stdout>:    - dropout : 0.1
[1,7]<stdout>:    - dropatt : 0.0
[1,7]<stdout>:    - init : normal
[1,7]<stdout>:    - emb_init : normal
[1,7]<stdout>:    - init_range : 0.1
[1,7]<stdout>:    - emb_init_range : 0.01
[1,7]<stdout>:    - init_std : 0.02
[1,7]<stdout>:    - proj_init_std : 0.01
[1,7]<stdout>:    - optim : adam
[1,7]<stdout>:    - lr : 0.00025
[1,7]<stdout>:    - mom : 0.0
[1,7]<stdout>:    - scheduler : cosine
[1,7]<stdout>:    - warmup_step : 0
[1,7]<stdout>:    - decay_rate : 0.5
[1,7]<stdout>:    - lr_min : 0.0
[1,7]<stdout>:    - clip : 0.25
[1,7]<stdout>:    - clip_nonemb : False
[1,7]<stdout>:    - max_step : 5000
[1,7]<stdout>:    - batch_size : 60
[1,7]<stdout>:    - batch_chunk : 1
[1,7]<stdout>:    - tgt_len : 150
[1,7]<stdout>:    - eval_tgt_len : 150
[1,7]<stdout>:    - ext_len : 0
[1,7]<stdout>:    - mem_len : 150
[1,7]<stdout>:    - not_tied : False
[1,7]<stdout>:    - seed : 1111
[1,7]<stdout>:    - cuda : True
[1,7]<stdout>:    - adaptive : True
[1,7]<stdout>:    - div_val : 1
[1,7]<stdout>:    - pre_lnorm : False
[1,7]<stdout>:    - varlen : False
[1,7]<stdout>:    - multi_gpu : False
[1,7]<stdout>:    - log_interval : 25
[1,7]<stdout>:    - eval_interval : 25
[1,7]<stdout>:    - work_dir : LM-TFM-wt103/20230830-005516
[1,7]<stdout>:    - restart : False
[1,7]<stdout>:    - restart_dir : 
[1,7]<stdout>:    - debug : False
[1,7]<stdout>:    - same_length : False
[1,7]<stdout>:    - attn_type : 0
[1,7]<stdout>:    - clamp_len : -1
[1,7]<stdout>:    - eta_min : 0.0
[1,7]<stdout>:    - gpu0_bsz : 4
[1,7]<stdout>:    - max_eval_steps : -1
[1,7]<stdout>:    - sample_softmax : -1
[1,7]<stdout>:    - patience : 0
[1,7]<stdout>:    - finetune_v2 : False
[1,7]<stdout>:    - finetune_v3 : False
[1,7]<stdout>:    - fp16 : False
[1,7]<stdout>:    - static_loss_scale : 1
[1,7]<stdout>:    - dynamic_loss_scale : False
[1,7]<stdout>:    - tied : True
[1,7]<stdout>:    - n_token : 267735
[1,7]<stdout>:    - n_all_param : 151107538
[1,7]<stdout>:    - n_nonemb_param : 41066400
[1,7]<stdout>:====================================================================================================
[1,7]<stdout>:#params = 151107538
[1,7]<stdout>:#non emb params = 41066400
[1,2]<stderr>:Traceback (most recent call last):
[1,2]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_actopk.py", line 418, in <module>
[1,3]<stderr>:Traceback (most recent call last):
[1,3]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_actopk.py", line 418, in <module>
[1,1]<stderr>:Traceback (most recent call last):
[1,1]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_actopk.py", line 418, in <module>
[1,5]<stderr>:Traceback (most recent call last):
[1,5]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_actopk.py", line 418, in <module>
[1,1]<stderr>:    hvd.broadcast_optimizer_state(optimizer, root_rank=0) 
[1,1]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/horovod/torch/functions.py", line 106, in broadcast_optimizer_state
[1,1]<stderr>:    super(optimizer.__class__, optimizer).step()
[1,1]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/optimizer.py", line 140, in wrapper
[1,1]<stderr>:    out = func(*args, **kwargs)
[1,1]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
[1,1]<stderr>:    ret = func(self, *args, **kwargs)
[1,1]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/adam.py", line 234, in step
[1,1]<stderr>:    adam(params_with_grad,
[1,1]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/adam.py", line 300, in adam
[1,1]<stderr>:    func(params,
[1,1]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/adam.py", line 410, in _single_tensor_adam
[1,1]<stderr>:    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)
[1,1]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 420.00 MiB (GPU 0; 31.74 GiB total capacity; 3.43 GiB already allocated; 16.12 MiB free; 3.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,7]<stderr>:Traceback (most recent call last):
[1,7]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_actopk.py", line 418, in <module>
[1,7]<stderr>:    hvd.broadcast_optimizer_state(optimizer, root_rank=0) 
[1,7]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/horovod/torch/functions.py", line 106, in broadcast_optimizer_state
[1,7]<stderr>:    super(optimizer.__class__, optimizer).step()
[1,7]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/optimizer.py", line 140, in wrapper
[1,7]<stderr>:    out = func(*args, **kwargs)
[1,7]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
[1,7]<stderr>:    ret = func(self, *args, **kwargs)
[1,7]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/adam.py", line 220, in step
[1,7]<stderr>:    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
[1,7]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 31.74 GiB total capacity; 2.83 GiB already allocated; 6.12 MiB free; 2.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,2]<stderr>:    hvd.broadcast_optimizer_state(optimizer, root_rank=0) 
[1,2]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/horovod/torch/functions.py", line 106, in broadcast_optimizer_state
[1,2]<stderr>:    super(optimizer.__class__, optimizer).step()
[1,2]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/optimizer.py", line 140, in wrapper
[1,2]<stderr>:    out = func(*args, **kwargs)
[1,2]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
[1,2]<stderr>:    ret = func(self, *args, **kwargs)
[1,2]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/adam.py", line 220, in step
[1,2]<stderr>:    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
[1,2]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 31.74 GiB total capacity; 2.75 GiB already allocated; 12.12 MiB free; 2.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,5]<stderr>:    hvd.broadcast_optimizer_state(optimizer, root_rank=0) 
[1,5]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/horovod/torch/functions.py", line 106, in broadcast_optimizer_state
[1,5]<stderr>:    super(optimizer.__class__, optimizer).step()
[1,5]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/optimizer.py", line 140, in wrapper
[1,5]<stderr>:    out = func(*args, **kwargs)
[1,5]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
[1,5]<stderr>:    ret = func(self, *args, **kwargs)
[1,5]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/adam.py", line 220, in step
[1,5]<stderr>:    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
[1,5]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 420.00 MiB (GPU 0; 31.74 GiB total capacity; 2.31 GiB already allocated; 406.12 MiB free; 2.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,6]<stdout>:====================================================================================================
[1,6]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,6]<stdout>:    - dataset : wt103
[1,6]<stdout>:    - n_layer : 16
[1,6]<stdout>:    - n_head : 10
[1,6]<stdout>:    - d_head : 41
[1,6]<stdout>:    - d_embed : 410
[1,6]<stdout>:    - d_model : 410
[1,6]<stdout>:    - d_inner : 2100
[1,6]<stdout>:    - dropout : 0.1
[1,6]<stdout>:    - dropatt : 0.0
[1,6]<stdout>:    - init : normal
[1,6]<stdout>:    - emb_init : normal
[1,6]<stdout>:    - init_range : 0.1
[1,6]<stdout>:    - emb_init_range : 0.01
[1,6]<stdout>:    - init_std : 0.02
[1,6]<stdout>:    - proj_init_std : 0.01
[1,6]<stdout>:    - optim : adam
[1,6]<stdout>:    - lr : 0.00025
[1,6]<stdout>:    - mom : 0.0
[1,6]<stdout>:    - scheduler : cosine
[1,6]<stdout>:    - warmup_step : 0
[1,6]<stdout>:    - decay_rate : 0.5
[1,6]<stdout>:    - lr_min : 0.0
[1,6]<stdout>:    - clip : 0.25
[1,6]<stdout>:    - clip_nonemb : False
[1,6]<stdout>:    - max_step : 5000
[1,6]<stdout>:    - batch_size : 60
[1,6]<stdout>:    - batch_chunk : 1
[1,6]<stdout>:    - tgt_len : 150
[1,6]<stdout>:    - eval_tgt_len : 150
[1,6]<stdout>:    - ext_len : 0
[1,6]<stdout>:    - mem_len : 150
[1,6]<stdout>:    - not_tied : False
[1,6]<stdout>:    - seed : 1111
[1,6]<stdout>:    - cuda : True
[1,6]<stdout>:    - adaptive : True
[1,6]<stdout>:    - div_val : 1
[1,6]<stdout>:    - pre_lnorm : False
[1,6]<stdout>:    - varlen : False
[1,6]<stdout>:    - multi_gpu : False
[1,6]<stdout>:    - log_interval : 25
[1,6]<stdout>:    - eval_interval : 25
[1,6]<stdout>:    - work_dir : LM-TFM-wt103/20230829-125516
[1,6]<stdout>:    - restart : False
[1,6]<stdout>:    - restart_dir : 
[1,6]<stdout>:    - debug : False
[1,6]<stdout>:    - same_length : False
[1,6]<stdout>:    - attn_type : 0
[1,6]<stdout>:    - clamp_len : -1
[1,6]<stdout>:    - eta_min : 0.0
[1,6]<stdout>:    - gpu0_bsz : 4
[1,6]<stdout>:    - max_eval_steps : -1
[1,6]<stdout>:    - sample_softmax : -1
[1,6]<stdout>:    - patience : 0
[1,6]<stdout>:    - finetune_v2 : False
[1,6]<stdout>:    - finetune_v3 : False
[1,6]<stdout>:    - fp16 : False
[1,6]<stdout>:    - static_loss_scale : 1
[1,6]<stdout>:    - dynamic_loss_scale : False
[1,6]<stdout>:    - tied : True
[1,6]<stdout>:    - n_token : 267735
[1,6]<stdout>:    - n_all_param : 151107538
[1,6]<stdout>:    - n_nonemb_param : 41066400
[1,6]<stdout>:====================================================================================================
[1,6]<stdout>:#params = 151107538
[1,6]<stdout>:#non emb params = 41066400
[1,3]<stderr>:    hvd.broadcast_optimizer_state(optimizer, root_rank=0) 
[1,3]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/horovod/torch/functions.py", line 106, in broadcast_optimizer_state
[1,3]<stderr>:    super(optimizer.__class__, optimizer).step()
[1,3]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/optimizer.py", line 140, in wrapper
[1,3]<stderr>:    out = func(*args, **kwargs)
[1,3]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
[1,3]<stderr>:    ret = func(self, *args, **kwargs)
[1,3]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/adam.py", line 218, in step
[1,3]<stderr>:    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
[1,3]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 31.74 GiB total capacity; 2.79 GiB already allocated; 12.12 MiB free; 2.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,6]<stderr>:Traceback (most recent call last):
[1,6]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_actopk.py", line 418, in <module>
[1,6]<stderr>:    hvd.broadcast_optimizer_state(optimizer, root_rank=0) 
[1,6]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/horovod/torch/functions.py", line 106, in broadcast_optimizer_state
[1,6]<stderr>:    super(optimizer.__class__, optimizer).step()
[1,6]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/optimizer.py", line 140, in wrapper
[1,6]<stderr>:    out = func(*args, **kwargs)
[1,6]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
[1,6]<stderr>:    ret = func(self, *args, **kwargs)
[1,6]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/adam.py", line 218, in step
[1,6]<stderr>:    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
[1,6]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 420.00 MiB (GPU 0; 31.74 GiB total capacity; 1.90 GiB already allocated; 104.12 MiB free; 1.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,4]<stderr>:    hvd.broadcast_optimizer_state(optimizer, root_rank=0) 
[1,4]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/horovod/torch/functions.py", line 106, in broadcast_optimizer_state
[1,4]<stderr>:    super(optimizer.__class__, optimizer).step()
[1,4]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/optimizer.py", line 140, in wrapper
[1,4]<stderr>:    out = func(*args, **kwargs)
[1,4]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
[1,4]<stderr>:    ret = func(self, *args, **kwargs)
[1,4]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/adam.py", line 220, in step
[1,4]<stderr>:    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
[1,4]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 420.00 MiB (GPU 0; 31.74 GiB total capacity; 2.31 GiB already allocated; 398.12 MiB free; 2.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,0]<stdout>:====================================================================================================
[1,0]<stdout>:    - data : /home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/data/wikitext-103
[1,0]<stdout>:    - dataset : wt103
[1,0]<stdout>:    - n_layer : 16
[1,0]<stdout>:    - n_head : 10
[1,0]<stdout>:    - d_head : 41
[1,0]<stdout>:    - d_embed : 410
[1,0]<stdout>:    - d_model : 410
[1,0]<stdout>:    - d_inner : 2100
[1,0]<stdout>:    - dropout : 0.1
[1,0]<stdout>:    - dropatt : 0.0
[1,0]<stdout>:    - init : normal
[1,0]<stdout>:    - emb_init : normal
[1,0]<stdout>:    - init_range : 0.1
[1,0]<stdout>:    - emb_init_range : 0.01
[1,0]<stdout>:    - init_std : 0.02
[1,0]<stdout>:    - proj_init_std : 0.01
[1,0]<stdout>:    - optim : adam
[1,0]<stdout>:    - lr : 0.00025
[1,0]<stdout>:    - mom : 0.0
[1,0]<stdout>:    - scheduler : cosine
[1,0]<stdout>:    - warmup_step : 0
[1,0]<stdout>:    - decay_rate : 0.5
[1,0]<stdout>:    - lr_min : 0.0
[1,0]<stdout>:    - clip : 0.25
[1,0]<stdout>:    - clip_nonemb : False
[1,0]<stdout>:    - max_step : 5000
[1,0]<stdout>:    - batch_size : 60
[1,0]<stdout>:    - batch_chunk : 1
[1,0]<stdout>:    - tgt_len : 150
[1,0]<stdout>:    - eval_tgt_len : 150
[1,0]<stdout>:    - ext_len : 0
[1,0]<stdout>:    - mem_len : 150
[1,0]<stdout>:    - not_tied : False
[1,0]<stdout>:    - seed : 1111
[1,0]<stdout>:    - cuda : True
[1,0]<stdout>:    - adaptive : True
[1,0]<stdout>:    - div_val : 1
[1,0]<stdout>:    - pre_lnorm : False
[1,0]<stdout>:    - varlen : False
[1,0]<stdout>:    - multi_gpu : False
[1,0]<stdout>:    - log_interval : 25
[1,0]<stdout>:    - eval_interval : 25
[1,0]<stdout>:    - work_dir : LM-TFM-wt103/20230829-125516
[1,0]<stdout>:    - restart : False
[1,0]<stdout>:    - restart_dir : 
[1,0]<stdout>:    - debug : False
[1,0]<stdout>:    - same_length : False
[1,0]<stdout>:    - attn_type : 0
[1,0]<stdout>:    - clamp_len : -1
[1,0]<stdout>:    - eta_min : 0.0
[1,0]<stdout>:    - gpu0_bsz : 4
[1,0]<stdout>:    - max_eval_steps : -1
[1,0]<stdout>:    - sample_softmax : -1
[1,0]<stdout>:    - patience : 0
[1,0]<stdout>:    - finetune_v2 : False
[1,0]<stdout>:    - finetune_v3 : False
[1,0]<stdout>:    - fp16 : False
[1,0]<stdout>:    - static_loss_scale : 1
[1,0]<stdout>:    - dynamic_loss_scale : False
[1,0]<stdout>:    - tied : True
[1,0]<stdout>:    - n_token : 267735
[1,0]<stdout>:    - n_all_param : 151107538
[1,0]<stdout>:    - n_nonemb_param : 41066400
[1,0]<stdout>:====================================================================================================
[1,0]<stdout>:#params = 151107538
[1,0]<stdout>:#non emb params = 41066400
[1,0]<stderr>:Traceback (most recent call last):
[1,0]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_actopk.py", line 418, in <module>
[1,0]<stderr>:    hvd.broadcast_optimizer_state(optimizer, root_rank=0) 
[1,0]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/horovod/torch/functions.py", line 106, in broadcast_optimizer_state
[1,0]<stderr>:    super(optimizer.__class__, optimizer).step()
[1,0]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/optimizer.py", line 140, in wrapper
[1,0]<stderr>:    out = func(*args, **kwargs)
[1,0]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
[1,0]<stderr>:    ret = func(self, *args, **kwargs)
[1,0]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/optim/adam.py", line 218, in step
[1,0]<stderr>:    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
[1,0]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 420.00 MiB (GPU 0; 31.74 GiB total capacity; 1.90 GiB already allocated; 314.12 MiB free; 1.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
[1,2]<stderr>:/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py:266: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/cuda/Indexing.cu:1435.)
[1,2]<stderr>:  attn_score = attn_score.float().masked_fill(
[1,5]<stderr>:/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py:266: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/cuda/Indexing.cu:1435.)
[1,5]<stderr>:  attn_score = attn_score.float().masked_fill(
[1,3]<stderr>:/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py:266: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/cuda/Indexing.cu:1435.)
[1,3]<stderr>:  attn_score = attn_score.float().masked_fill(
[1,6]<stderr>:/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py:266: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/cuda/Indexing.cu:1435.)
[1,6]<stderr>:  attn_score = attn_score.float().masked_fill(
[1,7]<stderr>:/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py:266: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/cuda/Indexing.cu:1435.)
[1,7]<stderr>:  attn_score = attn_score.float().masked_fill(
[1,7]<stderr>:/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py:266: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/cuda/Indexing.cu:1435.)
[1,7]<stderr>:  attn_score = attn_score.float().masked_fill(
[1,4]<stderr>:/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py:266: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/cuda/Indexing.cu:1435.)
[1,4]<stderr>:  attn_score = attn_score.float().masked_fill(
[1,4]<stderr>:/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py:266: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/cuda/Indexing.cu:1435.)
[1,4]<stderr>:  attn_score = attn_score.float().masked_fill(
[1,1]<stderr>:/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py:266: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/cuda/Indexing.cu:1435.)
[1,1]<stderr>:  attn_score = attn_score.float().masked_fill(
[1,6]<stderr>:/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py:266: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/cuda/Indexing.cu:1435.)
[1,6]<stderr>:  attn_score = attn_score.float().masked_fill(
[1,1]<stderr>:/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py:266: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/cuda/Indexing.cu:1435.)
[1,1]<stderr>:  attn_score = attn_score.float().masked_fill(
[1,3]<stderr>:/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py:266: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/cuda/Indexing.cu:1435.)
[1,3]<stderr>:  attn_score = attn_score.float().masked_fill(
[1,0]<stderr>:/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py:266: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/cuda/Indexing.cu:1435.)
[1,0]<stderr>:  attn_score = attn_score.float().masked_fill(
[1,2]<stderr>:/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py:266: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/cuda/Indexing.cu:1435.)
[1,2]<stderr>:  attn_score = attn_score.float().masked_fill(
[1,5]<stderr>:/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py:266: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/cuda/Indexing.cu:1435.)
[1,5]<stderr>:  attn_score = attn_score.float().masked_fill(
[1,6]<stderr>:Traceback (most recent call last):
[1,6]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_gtopk.py", line 591, in <module>
[1,6]<stderr>:    train()
[1,6]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_gtopk.py", line 485, in train
[1,6]<stderr>:    ret = para_model(data, target, *mems)
[1,6]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,6]<stderr>:    return forward_call(*input, **kwargs)
[1,6]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 745, in forward
[1,6]<stderr>:    hidden, new_mems = self._forward(data, mems=mems)
[1,6]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 676, in _forward
[1,6]<stderr>:    core_out = layer(core_out, pos_emb, self.r_w_bias,
[1,6]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,6]<stderr>:    return forward_call(*input, **kwargs)
[1,6]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 425, in forward
[1,6]<stderr>:    output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,
[1,6]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,6]<stderr>:    return forward_call(*input, **kwargs)
[1,6]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 274, in forward
[1,6]<stderr>:    attn_vec = torch.einsum('ijbn,jbnd->ibnd', (attn_prob, w_head_v))
[1,6]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/functional.py", line 373, in einsum
[1,6]<stderr>:    return einsum(equation, *_operands)
[1,6]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/functional.py", line 378, in einsum
[1,6]<stderr>:    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
[1,6]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 31.74 GiB total capacity; 5.13 GiB already allocated; 16.12 MiB free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,0]<stderr>:/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py:266: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/cuda/Indexing.cu:1435.)
[1,0]<stderr>:  attn_score = attn_score.float().masked_fill(
[1,6]<stderr>:Traceback (most recent call last):
[1,6]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train.py", line 591, in <module>
[1,6]<stderr>:    train()
[1,6]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train.py", line 485, in train
[1,6]<stderr>:    ret = para_model(data, target, *mems)
[1,6]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,6]<stderr>:    return forward_call(*input, **kwargs)
[1,6]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 745, in forward
[1,6]<stderr>:    hidden, new_mems = self._forward(data, mems=mems)
[1,6]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 676, in _forward
[1,6]<stderr>:    core_out = layer(core_out, pos_emb, self.r_w_bias,
[1,6]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,6]<stderr>:    return forward_call(*input, **kwargs)
[1,6]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 425, in forward
[1,6]<stderr>:    output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,
[1,6]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,6]<stderr>:    return forward_call(*input, **kwargs)
[1,6]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 266, in forward
[1,6]<stderr>:    attn_score = attn_score.float().masked_fill(
[1,6]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 31.74 GiB total capacity; 4.72 GiB already allocated; 16.12 MiB free; 4.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,2]<stderr>:Traceback (most recent call last):
[1,2]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_gtopk.py", line 591, in <module>
[1,2]<stderr>:    train()
[1,2]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_gtopk.py", line 485, in train
[1,2]<stderr>:    ret = para_model(data, target, *mems)
[1,2]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,2]<stderr>:    return forward_call(*input, **kwargs)
[1,2]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 745, in forward
[1,2]<stderr>:    hidden, new_mems = self._forward(data, mems=mems)
[1,2]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 676, in _forward
[1,2]<stderr>:    core_out = layer(core_out, pos_emb, self.r_w_bias,
[1,2]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,2]<stderr>:    return forward_call(*input, **kwargs)
[1,2]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 425, in forward
[1,2]<stderr>:    output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,
[1,2]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,2]<stderr>:    return forward_call(*input, **kwargs)
[1,2]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 274, in forward
[1,2]<stderr>:    attn_vec = torch.einsum('ijbn,jbnd->ibnd', (attn_prob, w_head_v))
[1,2]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/functional.py", line 373, in einsum
[1,2]<stderr>:    return einsum(equation, *_operands)
[1,2]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/functional.py", line 378, in einsum
[1,2]<stderr>:    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
[1,2]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 31.74 GiB total capacity; 6.19 GiB already allocated; 12.12 MiB free; 6.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,4]<stderr>:Traceback (most recent call last):
[1,4]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_gtopk.py", line 591, in <module>
[1,4]<stderr>:    train()
[1,4]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_gtopk.py", line 485, in train
[1,4]<stderr>:    ret = para_model(data, target, *mems)
[1,4]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,4]<stderr>:    return forward_call(*input, **kwargs)
[1,4]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 745, in forward
[1,4]<stderr>:    hidden, new_mems = self._forward(data, mems=mems)
[1,4]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 676, in _forward
[1,4]<stderr>:    core_out = layer(core_out, pos_emb, self.r_w_bias,
[1,4]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,5]<stderr>:Traceback (most recent call last):
[1,5]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_gtopk.py", line 591, in <module>
[1,4]<stderr>:    return forward_call(*input, **kwargs)
[1,4]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 425, in forward
[1,4]<stderr>:    output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,
[1,4]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,5]<stderr>:    train()
[1,5]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_gtopk.py", line 485, in train
[1,5]<stderr>:    ret = para_model(data, target, *mems)
[1,5]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,4]<stderr>:    return forward_call(*input, **kwargs)
[1,4]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 266, in forward
[1,4]<stderr>:    attn_score = attn_score.float().masked_fill(
[1,4]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 31.74 GiB total capacity; 5.07 GiB already allocated; 6.12 MiB free; 5.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,5]<stderr>:    return forward_call(*input, **kwargs)
[1,5]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 745, in forward
[1,5]<stderr>:    hidden, new_mems = self._forward(data, mems=mems)
[1,5]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 676, in _forward
[1,5]<stderr>:    core_out = layer(core_out, pos_emb, self.r_w_bias,
[1,5]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,5]<stderr>:    return forward_call(*input, **kwargs)
[1,5]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 425, in forward
[1,5]<stderr>:    output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,
[1,5]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,5]<stderr>:    return forward_call(*input, **kwargs)
[1,5]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 266, in forward
[1,5]<stderr>:    attn_score = attn_score.float().masked_fill(
[1,5]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 31.74 GiB total capacity; 6.14 GiB already allocated; 14.12 MiB free; 6.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,5]<stderr>:Traceback (most recent call last):
[1,5]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train.py", line 591, in <module>
[1,5]<stderr>:    train()
[1,5]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train.py", line 485, in train
[1,5]<stderr>:    ret = para_model(data, target, *mems)
[1,5]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,5]<stderr>:    return forward_call(*input, **kwargs)
[1,5]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 745, in forward
[1,5]<stderr>:    hidden, new_mems = self._forward(data, mems=mems)
[1,5]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 676, in _forward
[1,5]<stderr>:    core_out = layer(core_out, pos_emb, self.r_w_bias,
[1,5]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,5]<stderr>:    return forward_call(*input, **kwargs)
[1,5]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 425, in forward
[1,5]<stderr>:    output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,
[1,5]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,5]<stderr>:    return forward_call(*input, **kwargs)
[1,5]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 266, in forward
[1,5]<stderr>:    attn_score = attn_score.float().masked_fill(
[1,5]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 31.74 GiB total capacity; 4.37 GiB already allocated; 14.12 MiB free; 4.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,4]<stderr>:Traceback (most recent call last):
[1,4]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train.py", line 591, in <module>
[1,2]<stderr>:Traceback (most recent call last):
[1,2]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train.py", line 591, in <module>
[1,4]<stderr>:    train()
[1,4]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train.py", line 485, in train
[1,2]<stderr>:    train()
[1,2]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train.py", line 485, in train
[1,4]<stderr>:    ret = para_model(data, target, *mems)
[1,4]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,2]<stderr>:    ret = para_model(data, target, *mems)
[1,2]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,4]<stderr>:    return forward_call(*input, **kwargs)
[1,4]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 745, in forward
[1,2]<stderr>:    return forward_call(*input, **kwargs)
[1,2]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 745, in forward
[1,4]<stderr>:    hidden, new_mems = self._forward(data, mems=mems)
[1,4]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 676, in _forward
[1,2]<stderr>:    hidden, new_mems = self._forward(data, mems=mems)
[1,2]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 676, in _forward
[1,2]<stderr>:    core_out = layer(core_out, pos_emb, self.r_w_bias,
[1,2]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,4]<stderr>:    core_out = layer(core_out, pos_emb, self.r_w_bias,
[1,4]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,4]<stderr>:    return forward_call(*input, **kwargs)
[1,4]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 425, in forward
[1,2]<stderr>:    return forward_call(*input, **kwargs)
[1,2]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 425, in forward
[1,2]<stderr>:    output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,
[1,2]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,4]<stderr>:    output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,
[1,4]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,4]<stderr>:    return forward_call(*input, **kwargs)
[1,4]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 266, in forward
[1,2]<stderr>:    return forward_call(*input, **kwargs)
[1,2]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 266, in forward
[1,2]<stderr>:    attn_score = attn_score.float().masked_fill(
[1,2]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 31.74 GiB total capacity; 4.37 GiB already allocated; 12.12 MiB free; 4.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,4]<stderr>:    attn_score = attn_score.float().masked_fill(
[1,4]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 31.74 GiB total capacity; 5.43 GiB already allocated; 6.12 MiB free; 5.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,3]<stderr>:Traceback (most recent call last):
[1,3]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train.py", line 591, in <module>
[1,3]<stderr>:    train()
[1,3]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train.py", line 485, in train
[1,3]<stderr>:    ret = para_model(data, target, *mems)
[1,3]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,3]<stderr>:    return forward_call(*input, **kwargs)
[1,3]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 745, in forward
[1,3]<stderr>:    hidden, new_mems = self._forward(data, mems=mems)
[1,3]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 676, in _forward
[1,3]<stderr>:    core_out = layer(core_out, pos_emb, self.r_w_bias,
[1,3]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,3]<stderr>:    return forward_call(*input, **kwargs)
[1,3]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 425, in forward
[1,3]<stderr>:    output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,
[1,3]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,3]<stderr>:    return forward_call(*input, **kwargs)
[1,3]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 274, in forward
[1,3]<stderr>:    attn_vec = torch.einsum('ijbn,jbnd->ibnd', (attn_prob, w_head_v))
[1,3]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/functional.py", line 373, in einsum
[1,3]<stderr>:    return einsum(equation, *_operands)
[1,3]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/functional.py", line 378, in einsum
[1,3]<stderr>:    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
[1,3]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 31.74 GiB total capacity; 4.77 GiB already allocated; 4.12 MiB free; 4.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,7]<stderr>:Traceback (most recent call last):
[1,7]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train.py", line 591, in <module>
[1,7]<stderr>:    train()
[1,7]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train.py", line 485, in train
[1,7]<stderr>:    ret = para_model(data, target, *mems)
[1,7]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,7]<stderr>:    return forward_call(*input, **kwargs)
[1,7]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 745, in forward
[1,7]<stderr>:    hidden, new_mems = self._forward(data, mems=mems)
[1,7]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 676, in _forward
[1,7]<stderr>:    core_out = layer(core_out, pos_emb, self.r_w_bias,
[1,7]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,7]<stderr>:    return forward_call(*input, **kwargs)
[1,7]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 425, in forward
[1,7]<stderr>:    output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,
[1,7]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,7]<stderr>:    return forward_call(*input, **kwargs)
[1,7]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 274, in forward
[1,7]<stderr>:    attn_vec = torch.einsum('ijbn,jbnd->ibnd', (attn_prob, w_head_v))
[1,7]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/functional.py", line 373, in einsum
[1,7]<stderr>:    return einsum(equation, *_operands)
[1,7]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/functional.py", line 378, in einsum
[1,7]<stderr>:    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
[1,7]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 31.74 GiB total capacity; 5.15 GiB already allocated; 14.12 MiB free; 5.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,3]<stderr>:Traceback (most recent call last):
[1,3]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_gtopk.py", line 591, in <module>
[1,3]<stderr>:    train()
[1,3]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_gtopk.py", line 485, in train
[1,3]<stderr>:    ret = para_model(data, target, *mems)
[1,3]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,3]<stderr>:    return forward_call(*input, **kwargs)
[1,3]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 745, in forward
[1,3]<stderr>:    hidden, new_mems = self._forward(data, mems=mems)
[1,3]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 676, in _forward
[1,3]<stderr>:    core_out = layer(core_out, pos_emb, self.r_w_bias,
[1,3]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,3]<stderr>:    return forward_call(*input, **kwargs)
[1,3]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 425, in forward
[1,3]<stderr>:    output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,
[1,3]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,3]<stderr>:    return forward_call(*input, **kwargs)
[1,3]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 274, in forward
[1,3]<stderr>:    attn_vec = torch.einsum('ijbn,jbnd->ibnd', (attn_prob, w_head_v))
[1,3]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/functional.py", line 373, in einsum
[1,3]<stderr>:    return einsum(equation, *_operands)
[1,3]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/functional.py", line 378, in einsum
[1,3]<stderr>:    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
[1,3]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 31.74 GiB total capacity; 5.83 GiB already allocated; 6.12 MiB free; 6.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,7]<stderr>:Traceback (most recent call last):
[1,7]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_gtopk.py", line 591, in <module>
[1,7]<stderr>:    train()
[1,7]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_gtopk.py", line 485, in train
[1,7]<stderr>:    ret = para_model(data, target, *mems)
[1,7]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,7]<stderr>:    return forward_call(*input, **kwargs)
[1,7]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 745, in forward
[1,7]<stderr>:    hidden, new_mems = self._forward(data, mems=mems)
[1,7]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 676, in _forward
[1,7]<stderr>:    core_out = layer(core_out, pos_emb, self.r_w_bias,
[1,7]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,7]<stderr>:    return forward_call(*input, **kwargs)
[1,7]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 425, in forward
[1,7]<stderr>:    output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,
[1,7]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,7]<stderr>:    return forward_call(*input, **kwargs)
[1,7]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 274, in forward
[1,7]<stderr>:    attn_vec = torch.einsum('ijbn,jbnd->ibnd', (attn_prob, w_head_v))
[1,7]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/functional.py", line 373, in einsum
[1,7]<stderr>:    return einsum(equation, *_operands)
[1,7]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/functional.py", line 378, in einsum
[1,7]<stderr>:    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
[1,7]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 31.74 GiB total capacity; 5.48 GiB already allocated; 14.12 MiB free; 5.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,0]<stderr>:Traceback (most recent call last):
[1,0]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train.py", line 591, in <module>
[1,0]<stderr>:    train()
[1,0]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train.py", line 485, in train
[1,0]<stderr>:    ret = para_model(data, target, *mems)
[1,0]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,0]<stderr>:    return forward_call(*input, **kwargs)
[1,0]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 745, in forward
[1,0]<stderr>:    hidden, new_mems = self._forward(data, mems=mems)
[1,0]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 676, in _forward
[1,0]<stderr>:    core_out = layer(core_out, pos_emb, self.r_w_bias,
[1,0]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,0]<stderr>:    return forward_call(*input, **kwargs)
[1,0]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 425, in forward
[1,0]<stderr>:    output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,
[1,0]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,0]<stderr>:    return forward_call(*input, **kwargs)
[1,0]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 252, in forward
[1,0]<stderr>:    rr_head_q = w_head_q + r_r_bias
[1,0]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 31.74 GiB total capacity; 4.25 GiB already allocated; 8.12 MiB free; 4.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,1]<stderr>:Traceback (most recent call last):
[1,1]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_gtopk.py", line 591, in <module>
[1,1]<stderr>:    train()
[1,1]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_gtopk.py", line 485, in train
[1,1]<stderr>:    ret = para_model(data, target, *mems)
[1,1]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,1]<stderr>:    return forward_call(*input, **kwargs)
[1,1]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 745, in forward
[1,1]<stderr>:    hidden, new_mems = self._forward(data, mems=mems)
[1,1]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 676, in _forward
[1,1]<stderr>:    core_out = layer(core_out, pos_emb, self.r_w_bias,
[1,1]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,1]<stderr>:    return forward_call(*input, **kwargs)
[1,1]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 425, in forward
[1,1]<stderr>:    output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,
[1,1]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,1]<stderr>:    return forward_call(*input, **kwargs)
[1,1]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 266, in forward
[1,1]<stderr>:    attn_score = attn_score.float().masked_fill(
[1,1]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 31.74 GiB total capacity; 5.78 GiB already allocated; 30.12 MiB free; 6.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,1]<stderr>:Traceback (most recent call last):
[1,1]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train.py", line 591, in <module>
[1,1]<stderr>:    train()
[1,1]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train.py", line 485, in train
[1,1]<stderr>:    ret = para_model(data, target, *mems)
[1,1]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,1]<stderr>:    return forward_call(*input, **kwargs)
[1,1]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 745, in forward
[1,0]<stderr>:Traceback (most recent call last):
[1,0]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_gtopk.py", line 591, in <module>
[1,1]<stderr>:    hidden, new_mems = self._forward(data, mems=mems)
[1,1]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 676, in _forward
[1,1]<stderr>:    core_out = layer(core_out, pos_emb, self.r_w_bias,
[1,1]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,0]<stderr>:    train()
[1,0]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/train_gtopk.py", line 485, in train
[1,1]<stderr>:    return forward_call(*input, **kwargs)
[1,1]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 425, in forward
[1,0]<stderr>:    ret = para_model(data, target, *mems)
[1,0]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,1]<stderr>:    output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,
[1,1]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,0]<stderr>:    return forward_call(*input, **kwargs)
[1,0]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 745, in forward
[1,1]<stderr>:    return forward_call(*input, **kwargs)
[1,1]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 266, in forward
[1,1]<stderr>:    attn_score = attn_score.float().masked_fill(
[1,0]<stderr>:    hidden, new_mems = self._forward(data, mems=mems)
[1,1]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 31.74 GiB total capacity; 5.43 GiB already allocated; 30.12 MiB free; 5.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1,0]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 676, in _forward
[1,0]<stderr>:    core_out = layer(core_out, pos_emb, self.r_w_bias,
[1,0]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,0]<stderr>:    return forward_call(*input, **kwargs)
[1,0]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 425, in forward
[1,0]<stderr>:    output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,
[1,0]<stderr>:  File "/home/user/anaconda3/envs/actopk/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
[1,0]<stderr>:    return forward_call(*input, **kwargs)
[1,0]<stderr>:  File "/home/user/eurosys23/workspace/ACTopk/examples/nlp/transformer/transformer-xl/pytorch/mem_transformer.py", line 266, in forward
[1,0]<stderr>:    attn_score = attn_score.float().masked_fill(
[1,0]<stderr>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 31.74 GiB total capacity; 5.78 GiB already allocated; 10.12 MiB free; 6.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[61423,1],7]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[61421,1],7]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[61420,1],4]
  Exit code:    1
--------------------------------------------------------------------------
[1,0]<stdout>:34.05907130241394
[1,0]<stdout>:| epoch   1 step      125 |    125 batches | lr 0.00025 | ms/batch 1885.71 | loss  6.46 | ppl   641.894
[1,0]<stdout>:----------------------------------------------------------------------------------------------------
[1,0]<stdout>:| Eval   5 at step      125 | time: 40.02s | valid loss  6.28 | valid ppl   532.469
[1,0]<stdout>:----------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: ubuntu
  Local PID:  2840806
  Peer host:  n15
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 0 with PID 1764269 on node n15 exited on signal 9 (Killed).
--------------------------------------------------------------------------
[ubuntu:1764062] 2 more processes have sent help message help-mpi-btl-tcp.txt / peer hung up
[ubuntu:1764062] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
